<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE" >	
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
  @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);
    /* Color scheme stolen from Sergey Karayev */
    a {
    /*color: #b60a1c;*/
    color: #1772d0;
    /*color: #bd0a36;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Roboto', sans-serif;
    font-size: 15px;
    font-weight: 300;
    }
    strong {
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    /*font-family: 'Avenir Next';*/
    font-size: 15px;
    font-weight: 400;
    }
    heading {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 24px;
    font-weight: 400;
    }
    papertitle {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 15px;
    font-weight:500;
    }
    name {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    font-weight: 400;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 140px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="media/preview.jpg">
  <title>Songyou Peng - Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src="script/functions.js"></script>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <!--<tr onmouseout="headshot_stop()" onmouseover="headshot_start()">-->
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Songyou Peng (彭崧猷)</name>
          <!--<br>
          ruthfong at robots dot ox dot ac dot uk--> 
        </p>
        <p>
          I am a Research Scientist at <a href="https://deepmind.google/"><strong>Google DeepMind</strong></a> in San Francisco, USA.
        </p> 
        <p>
          I received my PhD from <a href="https://ethz.ch/en.html">ETH Zurich</a> and <a href="https://is.tuebingen.mpg.de/">Max Planck Institute for Intelligent Systems</a> under the supervision of <a href="https://inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a> and <a href="http://www.cvlibs.net/">Andreas Geiger</a>. After the PhD, I worked as a Senior Researcher/Postdoc at <a href="https://ethz.ch/en.html">ETH Zurich</a> for 6 months.
          <!-- I received my PhD from <a href="https://ethz.ch/en.html">ETH Zurich</a> and <a href="https://is.tuebingen.mpg.de/">Max Planck Institute for Intelligent Systems</a> under <a href="https://learning-systems.org/">Max Planck ETH Center for Learning Systems</a> & <a href="https://ellis.eu/">ELLIS</a>. I was co-supervised by <a href="https://inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a> and <a href="http://www.cvlibs.net/">Andreas Geiger</a>.  -->
          <!-- I am a PhD student at <a href="https://ethz.ch/en.html">ETH Zurich</a> and <a href="https://is.tuebingen.mpg.de/">Max Planck Institute for Intelligent Systems</a> under <a href="https://learning-systems.org/">Max Planck ETH Center for Learning Systems</a> & <a href="https://ellis.eu/">ELLIS</a>. I am co-supervised by <a href="https://inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a> and <a href="http://www.cvlibs.net/">Andreas Geiger</a>.  -->
          <!-- I am currently working at <a href="http://www.cvlibs.net/">Autonomous Vision Group (AVG)</a> at MPI Tubingen. -->
          <!-- I am currently working at <a href="https://cvg.ethz.ch">Computer Vision & Geometry Group</a> at ETH Zurich. -->
        </p>  
        <p>
          I was a research intern at <a href="https://research.google">Google Research</a> with <a href="https://research.google/people/ThomasFunkhouser/">Tom Funkhouser</a>, <a href="https://about.facebook.com/meta/">Meta</a> <a href="https://about.facebook.com/realitylabs/">Reality Labs Research</a> with <a href="https://zollhoefer.com/">Michael Zollhoefer</a>, <a href="https://vision.in.tum.de">Technical University of Munich</a> with <a href="https://vision.in.tum.de/members/cremers">Daniel Cremers</a>, and <a href="https://www.inria.fr/en/">INRIA</a> with <a href="https://team.inria.fr/steep/en/people/peter-sturm/">Peter Sturm</a>.
          I completed an Erasmus Mundus Masters in Computer Vision and Robotics (<a href="https://www.vibot.org/">VIBOT</a>) with distinction, and a Bachelors in Automation at <a href="http://en.xjtu.edu.cn/">Xi'an Jiaotong University</a>.
          <!-- During my PhD I was a research intern at <a href="https://research.google">Google Research</a> working with <a href="https://research.google/people/ThomasFunkhouser/">Tom Funkhouser</a>, and <a href="https://about.facebook.com/meta/">Meta</a> <a href="https://about.facebook.com/realitylabs/">Reality Labs Research</a>, hosted by <a href="https://zollhoefer.com/">Michael Zollhoefer</a>.
          I completed an Erasmus Mundus Masters in Computer Vision and Robotics (<a href="https://www.vibot.org/">VIBOT</a>) with distinction. During the master, I was fortunate to be supervised by <a href="https://vision.in.tum.de/members/cremers">Daniel Cremers</a> at <a href="https://vision.in.tum.de">Technical University of Munich</a> for my thesis and work with <a href="https://team.inria.fr/steep/en/people/peter-sturm/">Peter Sturm</a> at <a href="https://www.inria.fr/en/">INRIA</a> for two summers. Before this, I obtained a Bachelors in Automation at <a href="http://en.xjtu.edu.cn/">Xi'an Jiaotong University</a>.
          I have also spent some time at <a href="https://adsc.illinois.edu">ADSC</a> and <a href="https://www.a-star.edu.sg/">A*STAR</a> in Singapore. -->
        </p>

        <!-- <p>
          I have also spent some time at <a href="https://adsc.illinois.edu">ADSC</a> and <a href="https://www.a-star.edu.sg/">A*STAR</a> in Singapore.
        </p> -->
        <!-- <span style="color:#ff0000;">I will finish my PhD in fall 2023, and I am actively looking for research scientist positions or exciting startup opportunities.</span> -->
        <p align=center>
          <a href="mailto:songyou@google.com">Email</a> &nbsp|&nbsp
          <a href="files/Songyou_CV.pdf">CV</a> &nbsp|&nbsp
          <!--<a href="files/ruth_fong_bio.txt">Biography</a> &nbsp/&nbsp-->
<!--           <a href="https://scholar.google.com/citations?user=39cUD3gAAAAJ">Google Scholar</a> &nbsp/&nbsp -->
          <a href="https://github.com/pengsongyou">GitHub</a> &nbsp|&nbsp
          <a href="https://scholar.google.com/citations?user=eNypkO0AAAAJ&hl=en">Google Scholar</a> &nbsp|&nbsp
          <a href="https://www.linkedin.com/in/songyou-peng-53717648/"> LinkedIn</a>
          &nbsp|&nbsp
          <a href="https://twitter.com/songyoupeng">Twitter</a>
        </p>
        </td>
        <td width="33%">
          <img src="media/profile.jpg" width="250" alt="headshot">
          <!--<div class="one">
          <div class="two" id="headshot_image"><img src="media/color_headshot.png" width="250" alt="headshot"></div>
          <img src="media/bw_headshot.png" width="250" alt="headshot">
          </div>
          <script type="text/javascript">
          function headshot_start() {
          document.getElementById('headshot_image').style.opacity = "1";
          }
          function headshot_stop() {
          document.getElementById('headshot_image').style.opacity = "0";
          }
          filters_stop()
          </script>-->
        </td>
        <!--<td width="33%">
        <img src="media/bw_headshot.png" width="250">
        <img src="media/color_headshot.png" width="250">
        </td>-->
      </tr>
      <!-- <td width="10%"><a href="adsc.illinois.edu"><img src="media/adsc_logo.png" width="100"></a></td>
      <td width="10%"><a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="100"></a></td>
      <td width="10%"><a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="100"></a></td>
      <td width="10%"><a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="50"></a></td> -->
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <!-- <td width="10%" valign="middle">
          <a href="https://ethz.ch/en.html"><img src="media/logo_deepmind.png" width="60"></a>
        </td> -->
        <td width="10%" valign="middle">
          <a href="https://ethz.ch/en.html"><img src="media/eth_logo.png" width="120"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://www.is.mpg.de/"><img src="media/mpi_logo.png" width="60"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://research.google/"><img src="media/google_logo.png" width="55"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://about.facebook.com/realitylabs/"><img src="media/frl_logo.png" width="60"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="90"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="60"></a>
        </td>	
        <td width="10%" valign="middle">
          <a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="35"></a></td>
        </td>
        <td width="10%" valign="middle">
          <a href="https://en.xjtu.edu.cn/"><img src="media/xjtu_logo.png" width="50"></a></td>
        </td>	
        
        <!-- <td width="10%" valign="middle">
          <a href="https://ethz.ch/en.html"><img src="media/eth_logo.png" width="120"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://www.is.mpg.de/"><img src="media/mpi_logo.png" width="80"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="100"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="70"></a>
        </td>	
        <td width="10%" valign="middle">
          <a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="40"></a></td>
        </td>
        <td width="10%" valign="middle">
          <a href="https://en.xjtu.edu.cn/"><img src="media/xjtu_logo.png" width="60"></a></td>
        </td>	 -->
      </tr>
      </table>




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <heading>News</heading>
            <ul>
              <li><strong>07/2025</strong> Three papers (<a href="https://boyangdeng.com/visual-chronicles/"><strong>Visual Chronicles</strong></a>, <a href="https://cl-splats.github.io/"><strong>CL-Splats</strong></a>, and <a href="https://wildgs-slam.github.io"><strong>SplatTalk</strong></a>) are accepted to <strong>ICCV 2025</strong>!</li>
              <li><strong>05/2025</strong> Invited to give talks on <a href="files/talk_stanford_2025.pdf"><strong>A "Splatacular" Year of 3D Reconstruction</strong></a> at Stanford University and KAIST (as a guest lecture). </li>
              <li><strong>02/2025</strong> 4 papers (<a href="https://haofeixu.github.io/depthsplat/"><strong>DepthSplat</strong></a>, <a href="https://promptda.github.io/"><strong>Prompt Depth Anything</strong></a>, <a href=https://zju3dv.github.io/free360><strong>Free360</strong></a>, <a href=https://wildgs-slam.github.io><strong>WildGS-SLAM</strong></a>) are accepted to <strong>CVPR 2025</strong>!</li>
              <li><strong>02/2025</strong> <img src="media/logo_nopo.jpg" width="20"><a href="https://noposplat.github.io/"><strong>NoPoSplat</strong></a> is accepted to <strong>ICLR 2025</strong> as an <strong><span style="color:#c20000;">Oral </span>(top 1.8%)</strong>!</li>
              <li><strong>12/2024</strong> I will serve as an Area Chair at <strong>ICCV 2025</strong> and <strong>ICML 2025</strong>.</li>
              <li><strong>11/2024</strong> I give lectures for an 11-week course <strong>GAMES003: 图形视觉科研基本素养 (How To Do Research in CV/CG)</strong>. <a href="https://pengsida.net/games003/">All lecture recordings and slides</a> are online (in Chinese).</li>
              <li><strong>10/2024</strong> My PhD thesis received the <img src="media/logo_award.png" width="20"><a href="https://www.ecva.net/index.php#awards"><strong>ECVA PhD Award</strong></a></strong></a>!</li>
              <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
              <div id="old_news" style="display: none;">
              <li><strong>09/2024</strong> <a href="https://wild-gaussians.github.io/"><strong>WildGaussians</strong></a> and <a href="https://andrehuang.github.io/renovate/"><strong>RENOVATE</strong></a> are accepted to <strong>NeurIPS 2024</strong>!</li>
              <li><strong>07/2024</strong> <a href="https://segment3d.github.io/"><strong>Segment3D</strong></a> is accepted to ECCV 2024! Also, I will be co-organizing two workshops: <a href="https://focus-workshop.github.io/">Foundation Models Creators Meet Users (FOCUS)</a> and <a href="https://opensun3d.github.io/">Open-Vocabulary 3D Scene Understanding (OpenSUN3D)</a>.</li>
              <li><strong>05/2024</strong> <b><span style="color:#c20000;">Career Update</span></b>: I start working as a research scientist at <a href="https://deepmind.google/"><strong>Google DeepMind</strong></a>!</li>
              
              <li><strong>03/2024</strong> Our papers <a href="https://rwn17.github.io/nerf-on-the-go/"><strong>NeRF <em>On-the-go</em></strong></a> and <a href="https://neural-edge-map.github.io/"><strong>3D Neural Edge Reconstruction</strong></a> are accepted to <strong>CVPR 2024</strong>! Congrats on the successful master theses of my amazing students at ETH Zurich, <a href="https://scholar.google.com/citations?user=A6d9VTAAAAAJ">Lei Li</a> and <a href="https://github.com/rwn17">Weining Ren</a>!</li>
              <li><strong>03/2024</strong> <a href="https://nicer-slam.github.io/"><strong>NICER-SLAM</strong></a> received the <img src="media/logo_award.png" width="20"><strong><span style="color:#c20000;">Best Paper Honorable Mention Award</span></strong> at <strong>3DV 2024</strong>! 
                <!-- Congrats to all co-authors, especially <a href="https://zzh2000.github.io/">Zihan</a>, who started this project during his bachelor and did such a wonderful job!!</li> -->
              <li><strong>03/2024</strong> Invited to give a talk on <a href="files/talk_hku.pdf">2D Magic in a 3D World</a> at Imperial College London and the University of Hong Kong. </li>
              <li><strong>11/2023</strong> <strong>I successfully defended my PhD</strong>! [<a href="files/Songyou_PhD_Thesis.pdf">Thesis</a>][<a href="files/songyou_phd_defense.pdf">Defense Slides</a>]</li>
              <li><strong>10/2023</strong> Our papers <img src="media/logo_nicer.png" width="20"><a href="https://nicer-slam.github.io"> <strong>NICER-SLAM</strong></a> and <a href="https://l1346792580123.github.io/nccsfs/"><strong>FastHuman</strong></a> are accepted to 3DV 2024.</li>
              <li><strong>07/2023</strong> I served as an Area Chair at <a href="https://3dvconf.github.io/2024/area-chairs/">3DV 2024</a>.</li>
              <li><strong>07/2023</strong> I received the <img src="media/logo_award.png" width="20"><a href="https://twitter.com/GMFarinella/status/1679889056266694666"><strong>Best Presentation Award</strong></a> at <a href="https://iplab.dmi.unict.it/icvss2023/">ICVSS 2023</a>!</li>
              <li><strong>07/2023</strong> Our paper <a href="https://primecai.github.io/diffdreamer">DiffDreamer</a> is accepted to ICCV 2023! Congrats Shengqu on a successful master thesis!</li>
              <li><strong>07/2023</strong> Invited to give a 90-min lecture at <a href="https://sgp2023.github.io/program/">SGP 2023</a> graduate school in <a href="files/talk_sgp.pdf">neural explicit-implicit representations!</a></li>  
              <li><strong>03/2023</strong> I Co-organize <a href="https://opensun3d.github.io/"><strong>OpenSUN3D</strong>: 1st Workshop on Open-Vocabulary 3D Scene Understanding</a> in <strong>ICCV 2023</strong>.</li>
              <li><strong>03/2023</strong> Our paper <img src="media/openscene/logo.png" width="20"><a href="https://pengsongyou.github.io/openscene"><strong>OpenScene</strong></a> from my internship at Google Research is accepted to <strong>CVPR 2023</strong>!</li>
              <li><strong>10/2022</strong> Invited to give a talk on large-scale scene reconstruction with NeRF at <a href="https://www.computationalimaging.org/">Stanford University</a> (<a href="files/talk_stanford.pdf">Slides</a>). </li>
              <li><strong>09/2022</strong> Our paper <a href="https://niujinshuchong.github.io/monosdf/"><strong>MonoSDF</strong></a> is accepted to <strong>NeurIPS 2022</strong>. </li>
              <li><strong>09/2022</strong> Invited to give a talk on neural rendering at <a href="https://research.adobe.com/">Adobe Research</a> (<a href="files/talk_adobe.pdf">Slides</a>). </li>
              <li><strong>06/2022</strong> This summer I will be a research intern at <a href="https://research.google">Google Research</a>. </li> 
              <li><strong>06/2022</strong> <strong><span style="color:#c20000;">1st place winner</span></strong> in partial object recovery and 2nd place overall of <a href="https://cvi2.uni.lu/sharp2022/">SHARP Challenge</a>! Congratulations on my students Lei, Zhizheng, Weining, and Liudi from 3D Vision course project at ETH Zurich!
              <li><strong>05/2022</strong> Selected as an <a href="https://cvpr2022.thecvf.com/outstanding-reviewers">outstanding reviewer</a> at CVPR 2022. </li>
              <li><strong>03/2022</strong> Our paper <img src="media/nice-slam/like.png" width="20"><a href="https://pengsongyou.github.io/nice-slam"><strong>NICE-SLAM</strong></a> is accepted to <strong>CVPR 2022</strong>! </li>
              <li><strong>02/2022</strong> Invited to talk about <a href="https://pengsongyou.github.io/sap">Shape As Points</a> at <a href="https://twitter.com/talking_papers">Talking Papers Podcast</a>. Great chat with <a href="https://www.itzikbs.com/">Yizhak Ben-Shabat</a>!</li>
              <li><strong>12/2021</strong> Gave a talk again this year at <a href="http://games-cn.org/games-webinar-20211230-214/">GAMES Seminar Series</a> on <a href="https://pengsongyou.github.io/sap">Shape As Points</a>.</li>
              <li><strong>09/2021</strong> Our <a href="https://pengsongyou.github.io/sap"><strong>Shape As Points</strong></a> is accepted to NeurIPS 2021 as <span style="color:#c20000;"><strong>oral presentation</strong></span> <strong>(top 0.6%)</strong>!</li>
              <li><strong>08/2021</strong> Join <a href="https://research.fb.com/category/augmented-reality-virtual-reality/">Facebook Reality Labs (FRL)</a> as a research intern this fall.</li>
              <li><strong>07/2021</strong> Two papers (<a href="https://moechsle.github.io/unisurf/">UNISURF</a> and <a href="https://creiser.github.io/kilonerf/">KiloNeRF</a>) are accepted to ICCV 2021! </li>    
              <li><strong>06/2021</strong> Gave a talk at <a href="http://games-cn.org/games-webinar-20210621-187/">GAMES Seminar Series</a> on <a href="files/Towards_Practical_Application_of_NeRF.pdf">Towards Practical Applications of NeRF</a>. </li>
              <li><strong>11/2020</strong>: A <a href="https://github.com/dsvilarkovic/dynamic_plane_convolutional_onet">master course project</a> that I advised on got accepted to WACV 2021.</li>
              <li><strong>08/2020</strong>: Start my 1-year stay at <a href="http://www.cvlibs.net/">Autonomous Vision Group (AVG)</a> at MPI Tübingen.</li>
              <li><strong>07/2020</strong> Our paper <a href="https://arxiv.org/abs/2003.04618">Convolutional Occupancy Networks</a> is accepted to ECCV 2020 as <strong>spotlight (top 5%)</strong>! </li>
              <li><strong>09/2019</strong> Start PhD journey at <a href="https://learning-systems.org/">Max Planck ETH Center for Learning Systems</a>!</li>
              
              <!-- <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
              <div id="old_news" style="display: none;"> -->

              <li><strong>07/2019</strong> Our paper <a href="https://arxiv.org/abs/1811.03264">Calibration Wizard</a> is accepted to ICCV 2019 as <strong>oral presentation (top 4.6%) </strong>.</li>
              <li><strong>06/2019</strong>: The extension of my master thesis got accepted to TPAMI 2019! 
            </div></div>
            </ul>
        </td>
      </tr>
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    

        <tr onmouseout="gemini_stop()" onmouseover="gemini_start()">  
          <td width="25%" style="padding-bottom: 0px;">
            <div class="one">
            <div class="two" id = 'gemini_shape'>
            <img src='media/logo_gemini.png' width="160" height="59"></div>
            <img src='media/logo_gemini.png' width="160" height="59"></div>
            </div>
            <script type="text/javascript">
            function gemini_start() { 
            document.getElementById('gemini_shape').style.opacity = "1";
            }
            function gemini_stop() { 
            document.getElementById('gemini_shape').style.opacity = "0"; 
            }
            gemini_stop()
            </script>
          </td>
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://gemini.google.com">
                <papertitle>
                  Gemini 2.5
                </papertitle>
              </a>
          <br>
              <strong>Google DeepMind</strong>
            <br>
            <br>
            <a href="https://arxiv.org/abs/2507.06261">tech report</a> | 
            <a href="https://gemini.google.com">website</a>
          </td>
        </tr>

        <tr onmouseout="chronicles_stop()" onmouseover="chronicles_start()">  
          <td width="25%">
              <div class="one">
                <div class="two" id = 'chronicles_shape'>
                  <img src='media/teaser_chronicles_2.jpg' width="160" height="160"></div>
                  <img src='media/teaser_chronicles_1.jpg' width="160" height="160"></div>
                  </div>
              <script type="text/javascript">
              function chronicles_start() { 
              document.getElementById('chronicles_shape').style.opacity = "1";
              }
              function chronicles_stop() { 
              document.getElementById('chronicles_shape').style.opacity = "0"; 
              }
              chronicles_stop()
              </script>
              </script>
            </td>
          <td valign="top" width="75%">
              <a href="https://boyangdeng.com/visual-chronicles/">
                <papertitle>
                  Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images
                </papertitle>
              </a>
          <br>
              <a href="https://boyangdeng.com/">Boyang Deng</a>,
              <strong>Songyou Peng*</strong>,
              <a href="https://www.kylegenova.com/">Kyle Genova</a>*,
              <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>,
              <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>,
              <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
          <br>
          <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025 <strong>(<span style="color:#c20000;">Highlight</span>)</strong>
            <br>
            <a href="https://arxiv.org/abs/2504.08727">paper</a> |
            <a href="https://boyangdeng.com/visual-chronicles/">project page</a>
            <p></p>
            We help you find "unusual" things and trends in NYC and SF, like 200+ abstract sculptures, see left for an example.
            <p></p>
          </td>
        </tr>

        <tr onmouseout="clsplats_stop()" onmouseover="clsplats_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'clsplats_shape'>
            <video  width="160" height="160" muted autoplay loop>
                <source src="media/teaser_clsplats.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video></div>
            <img src='media/teaser_clsplats.jpg' width="160" height="160"></div>
            <script type="text/javascript">
            function clsplats_start() { 
            document.getElementById('clsplats_shape').style.opacity = "1";
            }
            function clsplats_stop() { 
            document.getElementById('clsplats_shape').style.opacity = "0"; 
            }
            clsplats_stop()
            </script>
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://boyangdeng.com/visual-chronicles/">
                <papertitle>
                  CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization
                </papertitle>
              </a>
          <br>
              <a href="https://janackermann.info/">Jan Ackermann</a>,
              <a href="https://jkulhanek.com/"> Jonas Kulhanek</a>,
              <a href="https://primecai.github.io/"> Shengqu Cai</a>,
              <a href="https://haofeixu.github.io/">Haofei Xu</a>,
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>,
              <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
              <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>,
              <strong>Songyou Peng</strong>
          <br>
          <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2506.21117">paper</a> |
            <a href="https://cl-splats.github.io/">project page</a> | 
            <a href="https://github.com/jan-ackermann/cl-splats">code</a> 
            <p></p>
            We give you great 3DGS even after you add, delete, change stuff in your room.
            <p></p>
          </td>
        </tr>
        
        <tr onmouseout="splattalk_stop()" onmouseover="splattalk_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'splattalk_shape'>
            <img src='media/teaser_splattalk.jpg' width="160" height="160"></div>
            <img src='media/teaser_splattalk.jpg' width="160" height="160"></div>
            </div>
            <script type="text/javascript">
            function splattalk_start() { 
            document.getElementById('splattalk_shape').style.opacity = "1";
            }
            function splattalk_stop() { 
            document.getElementById('splattalk_shape').style.opacity = "0"; 
            }
            splattalk_stop()
            </script>
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://splat-talk.github.io/">
                <papertitle>
                  SplatTalk: 3D VQA with Gaussian Splatting
                </papertitle>
              </a>
          <br>
              <a href="https://anhthai1997.com/">Anh Thai</a>,
              <strong>Songyou Peng</strong>,
              <a href="https://www.kylegenova.com/">Kyle Genova</a>,
              <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>,
              <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
          <br>
          <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2503.06271">paper</a> |
            <a href="https://splat-talk.github.io/">project page</a>
            <p></p>
            3D language Gaussian field benefits 3D VQA tasks.
            <p></p>
          </td>
        </tr>

        <tr onmouseout="depthsplat_stop()" onmouseover="depthsplat_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'depthsplat_shape'>
            <video  width="160" height="160" muted autoplay loop>
                <source src="media/teaser_depthsplat.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video></div>
            <img src='media/teaser_depthsplat.jpg' width="160" height="160"></div>
            <script type="text/javascript">
            function depthsplat_start() { 
            document.getElementById('depthsplat_shape').style.opacity = "1";
            }
            function depthsplat_stop() { 
            document.getElementById('depthsplat_shape').style.opacity = "0"; 
            }
            depthsplat_stop()
            </script>
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://haofeixu.github.io/depthsplat/">
                <papertitle>
                  DepthSplat: Connecting Gaussian Splatting and Depth
                </papertitle>
              </a>
          <br>
              <a href="https://haofeixu.github.io/">Haofei Xu</a>,
              <strong>Songyou Peng</strong>,
              <a href="https://fangjinhuawang.github.io/">Fangjinhua Wang</a>,
              <a href="https://hermannblum.net/">Hermann Blum</a>,
              <a href="https://scholar.google.com/citations?user=U9-D8DYAAAAJ">Daniel Barath</a>,
              <a href="https://www.cvlibs.net/">Andreas Geiger</a>,
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>
          <br>
          <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2410.13862">paper</a> |
            <a href="https://haofeixu.github.io/depthsplat/">project page</a> |
            <!-- <a href="https://youtu.be/H4cOCa3oUno?si=Vp60sexxbZIjwTMH">video</a> | -->
            <a href="https://github.com/cvg/depthsplat">code</a>
            <p></p>
            Depths helps 3DGS, 3DGS helps depth prediction.
            <p></p>
          </td>
        </tr>

        <tr onmouseout="pda_stop()" onmouseover="pda_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'pda_shape'>
            <video  width="160" height="160" muted autoplay loop>
                <source src="media/teaser_pda.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video></div>
            <img src='media/teaser_pda.jpg' width="160" height="160"></div>
            <script type="text/javascript">
            function pda_start() { 
            document.getElementById('pda_shape').style.opacity = "1";
            }
            function pda_stop() { 
            document.getElementById('pda_shape').style.opacity = "0"; 
            }
            pda_stop()
            </script>
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://promptda.github.io/">
                <papertitle>
                  Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation
                </papertitle>
              </a>
          <br>
              <a href="https://haotongl.github.io/">Haotong Lin</a>,
              <a href="https://pengsida.net/">Sida Peng</a>,
              <a href="https://timerchen.github.io/">Jingxiao Chen</a>,
              <strong>Songyou Peng</strong>
              <a href="https://jiamingsun.me/">Jiaming Sun</a>,
              <a href="https://minghuanliu.com/">Minghuan Liu</a>,
              <a href="http://www.cad.zju.edu.cn/home/bao/">Hujun Bao</a>,
              <a href="https://scholar.google.com/citations?user=Q8iay0gAAAAJ">Jiashi Feng</a>,
              <a href="https://www.xzhou.me/">Xiaowei Zhou</a>,
              <a href="https://bingykang.github.io/">Bingyi Kang</a>
          <br>
          <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2410.13862">paper</a> |
            <a href="https://haofeixu.github.io/depthsplat/">project page</a> |
            <!-- <a href="https://youtu.be/H4cOCa3oUno?si=Vp60sexxbZIjwTMH">video</a> | -->
            <a href="https://github.com/cvg/depthsplat">code</a>
            <p></p>
            4K accurate metric depth estimation from low-res LiDAR.
            <p></p>
          </td>
        </tr>

        <tr onmouseout="free360_stop()" onmouseover="free360_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'free360_shape'>
            <video  width="160" height="160" muted autoplay loop>
                <source src="media/teaser_free360.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video></div>
            <img src='media/teaser_free360.jpg' width="160" height="160"></div>
            <script type="text/javascript">
            function free360_start() { 
            document.getElementById('free360_shape').style.opacity = "1";
            }
            function free360_stop() { 
            document.getElementById('free360_shape').style.opacity = "0"; 
            }
            free360_stop()
            </script>
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://zju3dv.github.io/free360/">
                <papertitle>
                  Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views
                </papertitle>
              </a>
          <br>
              <a href="https://chobao.github.io/">Chong Bao</a>,
              <a href="https://niujinshuchong.github.io/">Zehao Yu</a>,
              <a href="https://scholar.google.com/citations?user=vvW-UNMAAAAJ&hl=en">Jiale Shi</a>,
              <a href="http://www.cad.zju.edu.cn/home/gfzhang/">Guofeng Zhang</a>,
              <strong>Songyou Peng</strong>,
              <a href="https://zhpcui.github.io/">Zhaopeng Cui</a>
          <br>
          <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2503.24382">paper</a> |
            <a href="https://zju3dv.github.io/free360/">project page</a> |
            <a href="https://youtu.be/hiSUzwdVHOY?si=HcN7XNKJAiyjkrOu">video</a> |
            <a href="https://github.com/chobao/Free360">code</a>
            <p></p>
            Video models enable unbounded 360° scene reconstruction from 3-4 unposed views.
            <p></p>
          </td>
        </tr>

        <tr onmouseout="wildgslam_stop()" onmouseover="wildgslam_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'wildgslam_shape'>
            <video  width="160" height="160" muted autoplay loop>
                <source src="media/teaser_wildgslam.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video></div>
            <img src='media/teaser_wildgslam.jpg' width="160" height="160"></div>
            <script type="text/javascript">
            function wildgslam_start() { 
            document.getElementById('wildgslam_shape').style.opacity = "1";
            }
            function wildgslam_stop() { 
            document.getElementById('wildgslam_shape').style.opacity = "0"; 
            }
            wildgslam_stop()
            </script>
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://wildgs-slam.github.io/">
                <papertitle>
                  WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments
                </papertitle>
              </a>
          <br>
              <a href="https://jianhao-zheng.github.io/">Jianhao Zheng</a>*,
              <a href="https://zzh2000.github.io/">Zihan Zhu</a>,
              <a href="https://ch.linkedin.com/in/valentin-bieri-98426b207">Valentin Bieri</a>,
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>,
              <strong>Songyou Peng</strong>,
              <a href="https://ir0.github.io/">Iro Armeni</a>
          <br>
          <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2504.03886">paper</a> |
            <a href="https://wildgs-slam.github.io/">project page</a> |
            <!-- <a href="https://youtu.be/H4cOCa3oUno?si=Vp60sexxbZIjwTMH">video</a> | -->
            <a href="https://github.com/GradientSpaces/WildGS-SLAM">code</a>
            <p></p>
            Robust SLAM for dynamic scenes in the wild.
            <p></p>
          </td>
        </tr>


        <tr onmouseout="nopo_stop()" onmouseover="nopo_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'nopo_shape'>
            <video  width="160" height="160" muted autoplay loop>
                <source src="media/teaser_nopo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video></div>
            <img src='media/teaser_nopo.jpg' width="160" height="160"></div>
            <script type="text/javascript">
            function nopo_start() { 
            document.getElementById('nopo_shape').style.opacity = "1";
            }
            function nopo_stop() { 
            document.getElementById('nopo_shape').style.opacity = "0"; 
            }
            nopo_stop()
            </script>
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://noposplat.github.io/">
                <papertitle>
                  <img src="media/logo_nopo.jpg" width="20"> No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images
                </papertitle>
              </a>
          <br>
              <a href="https://botaoye.github.io/">Botao Ye</a>,
              <a href="https://sifeiliu.net/">Sifei Liu</a>,
              <a href="https://haofeixu.github.io/">Haofei Xu</a>,
              <a href="https://sunshineatnoon.github.io/">Xueting Li</a>,
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>,
              <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>,
              <strong>Songyou Peng</strong>
          <br>
          <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025  <strong>(<span style="color:#c20000;">Oral</span>, top 1.8%)</strong>
            <br>
            <a href="https://arxiv.org/abs/2410.24207">paper</a> |
            <a href="https://noposplat.github.io/">project page</a> |
            <!-- <a href="https://youtu.be/H4cOCa3oUno?si=Vp60sexxbZIjwTMH">video</a> | -->
            <a href="https://github.com/cvg/NoPoSplat">code</a>
            <p></p>
            Unposed 3DGS made easy, also enables SoTA relative pose estimation performance!
            <p></p>
          </td>
        </tr>
    
        <tr onmouseout="wildgaussian_stop()" onmouseover="wildgaussian_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'wildgaussian_shape'>
            <video  width="160" height="120" muted autoplay loop>
                <source src="media/teaser_wildgaussians.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video></div>
            <img src='media/teaser_wildgaussians.jpg' width="160" height="120"></div>
            <script type="text/javascript">
            function wildgaussian_start() { 
            document.getElementById('wildgaussian_shape').style.opacity = "1";
            }
            function wildgaussian_stop() { 
            document.getElementById('wildgaussian_shape').style.opacity = "0"; 
            }
            wildgaussian_stop()
            </script>
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://wild-gaussians.github.io/">
                <papertitle>
                  WildGaussians: 3D Gaussian Splatting in the Wild
                </papertitle>
              </a>
          <br>
              <a href="https://jkulhanek.com/">Jonas Kulhanek</a>,
              <strong>Songyou Peng</strong>,
              <a href="https://cmp.felk.cvut.cz/~kukelova/">Zuzana Kukelova</a>,
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>,
              <a href="https://tsattler.github.io/">Torsten Sattler</a>
          <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
            <br>
            <a href="https://arxiv.org/abs/2407.08447">paper</a> |
            <a href="https://wild-gaussians.github.io/">project page</a> |
            <!-- <a href="https://youtu.be/H4cOCa3oUno?si=Vp60sexxbZIjwTMH">video</a> | -->
            <a href="https://github.com/jkulhanek/wild-gaussians/">code</a>
            <p></p>
            Boost 3DGS for in-the-wild scenes with appearance and dynamic changes.
            <p></p>
          </td>
        </tr>
    

        <tr onmouseout="renovate_stop()" onmouseover="renovate_start()">  
            <td width="25%">
              <div class="one">
                <div class="two" id = 'renovate_shape'>
                  <img src='media/renovate_2.jpeg' width="160" height="120"></div>
                  <img src='media/renovate_1.jpeg' width="160" height="120"></div>
                  </div>
              <script type="text/javascript">
              function renovate_start() { 
              document.getElementById('renovate_shape').style.opacity = "1";
              }
              function renovate_stop() { 
              document.getElementById('renovate_shape').style.opacity = "0"; 
              }
              renovate_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
                <a href="https://andrehuang.github.io/renovate/">
                  <papertitle>
                    Renovating Names in Open-Vocabulary Segmentation Benchmarks
                  </papertitle>
                </a>
            <br>
                <a href="https://andrehuang.github.io/">Haiwen Huang</a>,
                <strong>Songyou Peng</strong>,
                <a href="https://www.bosch-ai.com/research/researcher-pages/t_overviewpage_133.html">Dan Zhang</a>,
                <a href="https://www.cvlibs.net/">Andreas Geiger</a>
            <br>
                <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.09593">paper</a> |
              <a href="https://andrehuang.github.io/renovate/">project page</a> |
              <!-- <a href="https://youtu.be/H4cOCa3oUno?si=Vp60sexxbZIjwTMH">video</a> | -->
              <a href="https://github.com/autonomousvision/renovate">code</a>
              <p></p>
              Wanna enhance your segmentation model or benchmark? Renovate names now!
              <p></p>
            </td>
          </tr>
    
        <tr onmouseout="segment3d_stop()" onmouseover="segment3d_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'segment3d_shape'>
              <img src='media/segment3d_output.jpg' width="160" height="124"></div>
            <img src='media/segment3d_input.jpg' width="160" height="124"></div>
            <script type="text/javascript">
            function segment3d_start() { 
            document.getElementById('segment3d_shape').style.opacity = "1";
            }
            function segment3d_stop() { 
            document.getElementById('segment3d_shape').style.opacity = "0"; 
            }
            segment3d_stop()
            </script>
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://segment3d.github.io">
                <papertitle>
                  Segment3D: Learning Fine-Grained Class-Agnostic 3D Segmentation without Manual Labels
                </papertitle>
              </a>
          <br>
              <a href="https://scholar.google.com/citations?user=ieN4b1QAAAAJ&hl=zh-CN&oi=sra">Rui Huang</a>,
              <strong>Songyou Peng</strong>,
              <a href="https://aycatakmaz.github.io/">Ayça Takmaz</a>,
              <a href="https://federicotombari.github.io/"> Federico Tombari</a>,
              <a href="https://inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a>,
              <a href="https://scholar.google.com/citations?user=rw6vWdcAAAAJ&hl=zh-CN">Shiji Song</a>,
              <a href="https://www.gaohuang.net/">Gao Huang</a>,
              <a href="https://francisengelmann.github.io/">Francis Engelmann</a>
          <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
            <br>
            <a href="https://arxiv.org/abs/2312.17232">paper</a> |
            <a href="https://segment3d.github.io">project page</a> |
            <a href="https://github.com/LeapLabTHU/Segment3D">code</a> |
            <a href="https://mix3d-demo.nekrasov.dev/segment3d/">demo</a>
            <p></p>
            A self-supervised segmentation approach that outperforms fully-supervised methods.
            <p></p>
          </td>
        </tr>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
        <tr onmouseout="otg_stop()" onmouseover="otg_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'otg_shape'>
            <video  width="160" height="120" muted autoplay loop>
                <source src="media/teaser_otg_1.5x.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video></div>
            <img src='media/teaser_otg3.jpg' width="160" height="120"></div>
            <script type="text/javascript">
            function otg_start() { 
            document.getElementById('otg_shape').style.opacity = "1";
            }
            function otg_stop() { 
            document.getElementById('otg_shape').style.opacity = "0"; 
            }
            otg_stop()
            </script>
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://rwn17.github.io/nerf-on-the-go/">
                <papertitle>
                  <img src="media/logo_otg.png" width="20"> NeRF <em>On-the-go</em>: Exploiting Uncertainty for Distractor-free NeRFs in the Wild
                </papertitle>
              </a>
          <br>
              <a href="https://github.com/rwn17">Weining Ren</a>*,
              <a href="https://zzh2000.github.io">Zihan Zhu</a>*,
              <a href="https://inf.ethz.ch/people/people-atoz/person-detail.MjY0ODc2.TGlzdC8zMDQsLTIxNDE4MTU0NjA=.html">Boyang Sun</a>,
              <a href="https://inf.ethz.ch/people/people-atoz/person-detail.Mjc4NTY0.TGlzdC8zMDQsLTIxNDE4MTU0NjA=.html">Jiaqi Chen</a>,
              <a href="https://inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a>,
              <strong>Songyou Peng</strong>
          <br>
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
          <br>
            (* equal contribution)
            <br>
            <a href="https://arxiv.org/abs/2405.18715">paper</a> |
            <a href="https://rwn17.github.io/nerf-on-the-go/">project page</a> |
            <a href="https://youtu.be/mUQ_LOyonB0">video</a> |
            <a href="https://github.com/cvg/nerf-on-the-go">code</a>
            <p></p>
            We enable robust novel view synthesis from casually captured in-the-wild images. <br>
            <span style="color:#c20000;">Master thesis project.</span>
            <p></p>
          </td>
        </tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
          <tr onmouseout="edge_stop()" onmouseover="edge_start()">  
            <td width="25%">
              <div class="one">
              <div class="two" id = 'edge_shape'>
              <video  width="160" height="106" muted autoplay loop>
                  <source src="media/emap_teaser.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                  </video></div>
              <img src='media/emap_teaser.jpg' width="160" height="106"></div>
              <script type="text/javascript">
              function edge_start() { 
              document.getElementById('edge_shape').style.opacity = "1";
              }
              function edge_stop() { 
              document.getElementById('edge_shape').style.opacity = "0"; 
              }
              edge_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
                <a href="https://neural-edge-map.github.io/">
                  <papertitle>
                    3D Neural Edge Reconstruction
                  </papertitle>
                </a>
            <br>
                <a href="https://github.com/rayeeli">Lei Li</a>,
                <strong>Songyou Peng</strong>,
                <a href="https://niujinshuchong.github.io/">Zehao Yu</a>,
                <a href="http://b1ueber2y.me/">Shaohui Liu</a>,
                <a href="https://rpautrat.github.io/">Rémi Pautrat</a>,
                <a href="https://ieeexplore.ieee.org/author/37085401381">Xiaochuan Yin</a>,
                <a href="https://inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a>
            <br>
                <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
            <br>
              <a href="https://arxiv.org/pdf/2405.19295">paper</a> |
              <a href="https://neural-edge-map.github.io/">project page</a> |
              <a href="https://youtu.be/ONXfu2b4Nug">video</a> |
              <a href="https://github.com/cvg/EMAP">code</a>
              <p></p>
              <em>The straight line belongs to men, the curved one to God.</em>-- Antonio Gaudi<br>
              <span style="color:#c20000;">Master thesis project.</span>
              <p></p>
            </td>
          </tr>



      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
          <tr onmouseout="boft_stop()" onmouseover="boft_start()">  
            <td width="25%">
              <div class="one">
              <div class="two" id = 'boft_shape'>
                <center>
                  <video  width="130" height="130" muted autoplay loop>
                      <source src="media/boft_teaser.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                      </video></center></div>
                      <p style="text-align:center;">
                  <img src='media/boft_teaser.jpg' width="130" height="130"></p></div>
              <script type="text/javascript">
              function boft_start() { 
              document.getElementById('boft_shape').style.opacity = "1";
              }
              function boft_stop() { 
              document.getElementById('boft_shape').style.opacity = "0"; 
              }
              boft_stop()
              </script>
            </td>
            <td valign="top" width="75%">
                <a href="https://boft.wyliu.com/">
                  <papertitle>
                    Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization
                  </papertitle>
                </a>
            <br>
            <a href="https://wyliu.com/">Weiyang Liu</a>*, 
            <a href="https://github.com/Zeju1997">Zeju Qiu</a>*, 
            <a href="https://github.com/yfeng95">Yao Feng</a>**, 
            <a href="https://xiuyuliang.cn/">Yuliang Xiu</a>**, 
            <a href="https://yuxuan-xue.com/">Yuxuan Xue</a>**, 
            <a href="https://yulonghui.github.io/">Longhui Yu</a>**,
            <a href="https://ps.is.mpg.de/person/hfeng">Haiwen Feng</a>, 
            <a href="https://itszhen.com/">Zhen Liu</a>, 
            <a href="https://sites.google.com/view/juyeonheo">Juyeon Heo</a>,
            <strong>Songyou Peng</strong>, 
            <a href="https://ydwen.github.io/">Yandong Wen</a>, 
            <a href="https://ps.is.mpg.de/person/black">Michael J. Black</a>, 
            <a href="https://mlg.eng.cam.ac.uk/adrian/">Adrian Weller</a>, 
            <a href="https://is.mpg.de/~bs">Bernhard Schölkopf</a>
            <br>
            (*/** equal contribution)
            <br>
            <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2311.06243">paper</a> |
              <a href="https://boft.wyliu.com/">project page</a> |
              <a href="https://github.com/wy1iu/butterfly-oft">code</a> |
              <a href="https://huggingface.co/docs/peft/main/en/conceptual_guides/oft"
                style="padding-left: 0.5rem; vertical-align: top">
                <img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-PEFT-orange" />
              </a>
              <p></p>
              BOFT (Orthogonal Butterfly) is a general finetuning technique that adapts foundation models to different tasks such as Vision, NLP, Math QA, and Controllable Generation.
              <p></p>
            </td>
          </tr>
  

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
        <tr onmouseout="nicer_stop()" onmouseover="nicer_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'nicer_shape'>
            <video  width="160" height="104" muted autoplay loop>
                <source src="media/nicer_teaser.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video></div>
            <img src='media/nicer_teaser.jpg' width="160" height="104"></div>
            <script type="text/javascript">
            function nicer_start() { 
            document.getElementById('nicer_shape').style.opacity = "1";
            }
            function nicer_stop() { 
            document.getElementById('nicer_shape').style.opacity = "0"; 
            }
      nicer_stop()
            </script>
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://nicer-slam.github.io">
                <papertitle>
                  <img src="media/logo_nicer.png" width="20"> NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM
                </papertitle>
              </a>
          <br>
              <a href="https://zzh2000.github.io">Zihan Zhu</a>*,
              <strong>Songyou Peng*</strong>,
              <a href="http://people.inf.ethz.ch/vlarsson/">Viktor Larsson</a>,
              <a href="https://zhpcui.github.io/"> Zhaopeng Cui</a>,
              <a href="http://people.inf.ethz.ch/moswald/"> Martin R. Oswald</a>,
              <a href="https://www.cvlibs.net/">Andreas Geiger</a>,
              <a href="https://inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a>
          <br>
              <em>International Conference on 3D Vision (<strong>3DV</strong>)</em>, 2024 (<b><span style="color:#c20000;">Oral, Best Paper Honorable Mention</span></b>)
          <br>
            (* equal contribution)
            <br>
            <a href="https://arxiv.org/abs/2302.03594">paper</a> |
            <a href="https://nicer-slam.github.io">project page</a> |
            <a href="https://youtu.be/H4cOCa3oUno?si=Vp60sexxbZIjwTMH">video</a> |
            <a href="https://github.com/cvg/nicer-slam">code</a>
            <p></p>
            <strong>RGB-only</strong> version of our <a href="https://pengsongyou.github.io/nice-slam">NICE-SLAM</a>, making it NICE<strong>R</strong>.
            <p></p>
          </td>
        </tr>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
        <tr onmouseout="patch_stop()" onmouseover="patch_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'patch_shape'>
              <center>
                <video  width="160" height="134" muted autoplay loop>
                    <source src="media/patch_teaser.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video></center></div>
                    <p style="text-align:center;">
                <img src='media/patch_teaser.jpg' width="160" height="134"></p></div>
            <script type="text/javascript">
            function patch_start() { 
            document.getElementById('patch_shape').style.opacity = "1";
            }
            function patch_stop() { 
            document.getElementById('patch_shape').style.opacity = "0"; 
            }
            patch_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://l1346792580123.github.io/nccsfs/">
                <papertitle>
                  FastHuman: Reconstructing High-Quality Clothed Human in Minutes
                </papertitle>
              </a>
          <br>
              <a href="https://github.com/l1346792580123/">Lixiang Lin</a>, 
              <strong>Songyou Peng</strong>, 
              <a href="https://www.google.com/search?q=Qijun+Gan+ZJU">Qijun Gan</a>, 
              <a href="https://jkzhu.github.io/">Jianke Zhu</a>
          <br>
              <em>International Conference on 3D Vision (<strong>3DV</strong>)</em>, 2024 <strong>(<span style="color:#c20000;">Spotlight</span>, top 8.2%)</strong>
            <br>
            <a href="https://arxiv.org/abs/2211.14485">paper</a> |
            <a href="https://l1346792580123.github.io/nccsfs/">project page</a> |
            <a href="https://github.com/l1346792580123/FastHuman">code</a>
            <p></p>
            Shape As Points (SAP) for fast human body reconstruction.
            <p></p>
          </td>
        </tr>

        <tr onmouseout="thesis_stop()" onmouseover="thesis_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'stanford_shape'>
            <img src='media/thesis_cover.jpg' width="160" height="145"></div>
            <img src='media/thesis_cover.jpg' width="160" height="145"></div>
            </div>
            <script type="text/javascript">
            function thesis_start() { 
            document.getElementById('thesis_shape').style.opacity = "1";
            }
            function thesis_stop() { 
            document.getElementById('thesis_shape').style.opacity = "0"; 
            }
            thesis_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="files/Songyou_PhD_Thesis.pdf">
                <papertitle>Neural Scene Representations for 3D Reconstruction and Scene Understanding</papertitle></a>
          <br>
              <strong>Songyou Peng</strong>
          <br>
              <em>PhD Thesis</em>, 2023 (<b><span style="color:#c20000;">ECVA PhD Award</span></b>)
          <br>
            <a href="files/Songyou_PhD_Thesis.pdf">thesis</a> |
            <a href="files/songyou_phd_defense.pdf">slides</a>
          <p></p>
          PhD supervisors: <a href="https://people.inf.ethz.ch/pomarc/">Prof. Marc Pollefeys</a> (ETH Zurich), <a href="https://www.cvlibs.net/">Prof. Andreas Geiger</a> (MPI-IS)<br>
          External committee: <a href="https://profiles.stanford.edu/leonidas-guibas">Prof. Leonidas J. Guibas</a> (Stanford), <a href="https://www.vincentsitzmann.com/">Prof. Vincent Sitzmann</a> (MIT)
          </td>
        </tr>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
          <tr onmouseout="dreamer_stop()" onmouseover="dreamer_start()">  
            <td width="25%">
              <div class="one">
              <div class="two" id = 'dreamer_shape'>
                <center>
              <video  width="140" height="140" muted autoplay loop>
                  <source src="media/dreamer_teaser.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                  </video></center></div>
                  <p style="text-align:center;">
              <img src='media/dreamer_teaser.jpg' width="140" height="140"></p></div>
              <script type="text/javascript">
              function dreamer_start() { 
              document.getElementById('dreamer_shape').style.opacity = "1";
              }
              function dreamer_stop() { 
              document.getElementById('dreamer_shape').style.opacity = "0"; 
              }
              dreamer_stop()
              </script>
            </td>
            <td valign="top" width="75%">
                <a href="https://primecai.github.io/diffdreamer">
                  <papertitle>
                    DiffDreamer: Towards Consistent Unsupervised Single-view Scene Extrapolation with Conditional Diffusion Models
                  </papertitle>
                </a>
            <br>
                <a href="https://primecai.github.io/">Shengqu Cai</a>,
                <a href="https://ericryanchan.github.io/">Eric R. Chan</a>, 
                <strong>Songyou Peng</strong>, 
                <a href="https://people.ee.ethz.ch/~mshahbazi/">Mohamad Shahbazi</a>, 
                <a href="https://www.obukhov.ai/">Anton Obukhov</a>,
                <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Luc Van Gool</a>,
                <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>
            <br>
                <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2211.12131">paper</a> |
              <a href="https://primecai.github.io/diffdreamer">project page</a> 
              <!-- <a href="https://youtu.be/V5hYTz5os0M">video</a> |
              <a href="https://github.com/cvg/nice-slam">code</a> -->
              <p></p>
              A diffusion-model based unsupervised framework capable of synthesizing novel views depicting a long camera trajectory.
              <p></p>
            </td>
          </tr>
      
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
        <tr onmouseout="openscene_stop()" onmouseover="openscene_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'openscene_shape'>
            <video  width="166" height="117" muted autoplay loop>
                <source src="media/openscene_teaser.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video></div>
            <img src='media/openscene_teaser2.jpg' width="166" height="117"></div>
            <script type="text/javascript">
            function openscene_start() { 
            document.getElementById('openscene_shape').style.opacity = "1";
            }
            function openscene_stop() { 
            document.getElementById('openscene_shape').style.opacity = "0"; 
            }
            openscene_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://pengsongyou.github.io/openscene">
                <papertitle>
                  <img src="media/openscene/logo.png" width="20"> OpenScene: 3D Scene Understanding with Open Vocabularies
                </papertitle>
              </a>
          <br>
              <strong>Songyou Peng</strong>, 
              <a href="https://www.kylegenova.com/">Kyle Genova</a>, 
              <a href="https://www.maxjiang.ml/">Chiyu "Max" Jiang</a>, 
              <a href="https://taiya.github.io/">Andrea Tagliasacchi</a>, 
              <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>,
              <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
          <br>
          <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2211.15654">paper</a> |
            <a href="https://pengsongyou.github.io/openscene">project page</a> |
            <a href="https://youtu.be/jZxCLHyDJf8">video</a> |
            <a href="https://github.com/pengsongyou/openscene">code</a>
            <p></p>
            Zero-shot approach for novel 3D scene understanding tasks with open-vocabulary queries.
            <p></p>
          </td>
        </tr>

        <tr onmouseout="sdfstudio_stop()" onmouseover="sdfstudio_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'sdfstudio_shape'>
            <video  width="166" height="108" muted autoplay loop>
                <source src="media/sdfstudio_garden.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video></div>
            <img src='media/sdfstudio_garden.jpg' width="166" height="108"></div>
            <script type="text/javascript">
            function sdfstudio_start() { 
            document.getElementById('sdfstudio_shape').style.opacity = "1";
            }
            function sdfstudio_stop() { 
            document.getElementById('sdfstudio_shape').style.opacity = "0"; 
            }
            sdfstudio_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://autonomousvision.github.io/sdfstudio/">
                <papertitle>
                  <img src="media/sdfstudio_logo.png" width="90">: A Unified Framework for Surface Reconstruction
                </papertitle>
              </a>
          <br>
          <a href="https://niujinshuchong.github.io/">Zehao Yu</a>, 
          <a href="https://apchenstu.github.io/">Anpei Chen</a>, 
          <a href="https://bozidarantic.com/">Bozidar Antic</a>, 
          <strong>Songyou Peng</strong>, 
          <a href="https://apratimbhattacharyya18.github.io/">Apratim Bhattacharyya</a>, 
          <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>, 
          <a href="https://vlg.inf.ethz.ch/">Siyu Tang</a>, 
          <a href="https://tsattler.github.io/">Torsten Sattler</a>, 
          <a href="http://cvlibs.net/">Andreas Geiger</a>
          <br>
          <em>Open Source Project, 2023</em> <iframe
          src="https://ghbtns.com/github-btn.html?user=autonomousvision&repo=sdfstudio&type=star&count=true&size=small"
          frameborder="0"
          scrolling="0"
          width="100"
          height="20"
        ></iframe>
            <br>
            <a href="https://autonomousvision.github.io/sdfstudio/">project page</a> |
            <a href="https://github.com/autonomousvision/sdfstudio">code</a>
            <p></p>
            We provide a unified framework and benchmark for neural implicit surface reconstruction.
            <p></p>
          </td>
        </tr>
  

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
        <tr onmouseout="mono_stop()" onmouseover="mono_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'mono_shape'>
            <video  width="160" height="108" muted autoplay loop>
                <source src="media/monosdf_teaser.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video></div>
            <img src='media/monosdf_teaser.jpg' width="160" height="108"></div>
            <script type="text/javascript">
            function mono_start() { 
            document.getElementById('mono_shape').style.opacity = "1";
            }
            function mono_stop() { 
            document.getElementById('mono_shape').style.opacity = "0"; 
            }
      mono_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://niujinshuchong.github.io/monosdf">
                <papertitle>
                  MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction
                </papertitle>
              </a>
          <br>
              <a href="https://niujinshuchong.github.io/">Zehao Yu</a>,
              <strong>Songyou Peng</strong>, 
              <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>, 
              <a href="https://tsattler.github.io/">Torsten Sattler</a>, 
              <a href="http://cvlibs.net/">Andreas Geiger</a>
          <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
            <br>
            <a href="https://arxiv.org/abs/2206.00665">paper</a> |
            <a href="https://niujinshuchong.github.io/monosdf">project page</a> 
            <!-- <a href="https://youtu.be/V5hYTz5os0M">video</a> |
            <a href="https://github.com/cvg/nice-slam">code</a> -->
            <p></p>
            Monocular depth and normal cues significantly boost the performance of neural implicit surface reconstruction methods.
            <p></p>
          </td>
        </tr>
    
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
        <tr onmouseout="nice_stop()" onmouseover="nice_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'nice_shape'>
            <video  width="160" height="90" muted autoplay loop>
                <source src="media/nice_teaser.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video></div>
            <img src='media/nice_teaser.jpg' width="160" height="90"></div>
            <script type="text/javascript">
            function nice_start() { 
            document.getElementById('nice_shape').style.opacity = "1";
            }
            function nice_stop() { 
            document.getElementById('nice_shape').style.opacity = "0"; 
            }
      nice_stop()
            </script>
            </script>
          </td>
          <td valign="top" width="75%">
              <a href="https://pengsongyou.github.io/nice-slam">
                <papertitle>
                  <img src="media/nice-slam/like.png" width="20"> NICE-SLAM: Neural Implicit Scalable Encoding for SLAM
                </papertitle>
              </a>
          <br>
              <a href="https://zzh2000.github.io">Zihan Zhu</a>*,
              <strong>Songyou Peng*</strong>,
              <a href="http://people.inf.ethz.ch/vlarsson/">Viktor Larsson</a>,
              <a href="http://www.cad.zju.edu.cn/home/weiweixu/weiweixu_en.htm"> Weiwei Xu</a>,
              <a href="http://www.cad.zju.edu.cn/home/bao/"> Hujun Bao</a>,
              <a href="https://zhpcui.github.io/"> Zhaopeng Cui</a>,
              <a href="http://people.inf.ethz.ch/moswald/"> Martin R. Oswald</a>,
              <a href="https://inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a>
          <br>
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
          <br>
            (* equal contribution)
            <br>
            <a href="https://arxiv.org/abs/2112.12130">paper</a> |
            <a href="https://pengsongyou.github.io/nice-slam">project page</a> |
            <a href="https://youtu.be/V5hYTz5os0M">video</a> |
            <a href="https://github.com/cvg/nice-slam">code</a>
            <p></p>
            A neural implicit-based RGB-D SLAM that can be applied to large-scale scenes.
            <p></p>
          </td>
        </tr>
    
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
      <tr onmouseout="sap_stop()" onmouseover="sap_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'sap_shape'>
        <video  width="160" height="120" muted autoplay loop>
            <source src="media/sap_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video></div>
        <img src='media/sap_teaser.jpg' width="160" height="120"></div>
        <script type="text/javascript">
        function sap_start() { 
        document.getElementById('sap_shape').style.opacity = "1";
        }
        function sap_stop() { 
        document.getElementById('sap_shape').style.opacity = "0"; 
        }
  sap_stop()
        </script>
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="https://pengsongyou.github.io/sap">
            <papertitle>
              Shape As Points: A Differentiable Poisson Solver
            </papertitle>
          </a>
      <br>
          <strong>Songyou Peng</strong>,
          <a href="https://www.maxjiang.ml/">Chiyu "Max" Jiang</a>,
          <a href="https://yiyiliao.github.io/">Yiyi Liao</a>,
          <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>,
          <a href="https://inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a>,
          <a href="http://www.cvlibs.net/">Andreas Geiger</a>
      <br>
          <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2021 <strong>(<span style="color:#c20000;">Oral</span>, top 0.6%)</strong>
      <br>
        <a href="https://arxiv.org/abs/2106.03452">paper</a> |
        <a href="https://pengsongyou.github.io/sap">project page</a> |
        <a href="https://youtu.be/FL8LMk_qWb4">video (6 min)</a> |
        <a href="https://youtu.be/TgR0NvYty0A">video (12 min)</a> |
        <a href="https://talking.papers.podcast.itzikbs.com/1914034/10135005-songyou-peng-shape-as-points">podcast</a> | 
        <a href="https://github.com/autonomousvision/shape_as_points">code</a>        
        <p></p>
        An interpretable hybird shape representation that yields HQ watertight meshes at low inference times.
        <p></p>
      </td>
    </tr>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    <tr onmouseout="unisurf_stop()" onmouseover="unisurf_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'unisurf_shape'>
        <img src='media/unisurf_shape2.jpg' width="160" height="130"></div>
        <img src='media/unisurf_rgb2.jpg' width="160" height="130"></div>
        </div>
        <script type="text/javascript">
        function unisurf_start() { 
        document.getElementById('unisurf_shape').style.opacity = "1";
        }
        function unisurf_stop() { 
        document.getElementById('unisurf_shape').style.opacity = "0"; 
        }
        unisurf_stop()
        </script>
        </script>
      </td>
      <td valign="top" width="75%">
            <a href="https://moechsle.github.io/unisurf/">
            <papertitle>UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction</papertitle></a>
      <br>
      	  <a href="https://avg.is.tuebingen.mpg.de/person/moechsle">Michael Oechsle</a>,
      	  <strong>Songyou Peng</strong>,
          <a href="http://www.cvlibs.net/">Andreas Geiger</a>
      <br>
          <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021  <strong>(<span style="color:#c20000;">Oral</span>, top 3%)</strong>
   
      <br>
        <a href="https://arxiv.org/abs/2104.10078">paper</a> |
	<a href="https://moechsle.github.io/unisurf/">project page</a> |
        <a href="https://youtu.be/WXUfHvZge0E">video</a> |
        <a href="https://youtu.be/OSHlNS6ytkc">teaser video</a> |
        <a href="https://github.com/autonomousvision/unisurf">code</a>
        <p></p>
          Our method enables to reconstruct accurate surfaces without input masks.
        <p></p>
      </td>
    </tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    <tr onmouseout="kilo_stop()" onmouseover="kilo_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'kilo_shape'>
        <video  width="160" height="130" muted autoplay loop>
            <source src="media/kilo_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video></div>
        <img src='media/kilo_teaser.jpg' width="150" height="120"></div>
        <script type="text/javascript">
        function kilo_start() { 
        document.getElementById('kilo_shape').style.opacity = "1";
        }
        function kilo_stop() { 
        document.getElementById('kilo_shape').style.opacity = "0"; 
        }
	kilo_stop()
        </script>
        </script>
      </td>
      <td valign="top" width="75%">
            <a href="https://creiser.github.io/kilonerf/">
            <papertitle>KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs</papertitle></a>
      <br>
      	  <a href="https://avg.is.tuebingen.mpg.de/person/creiser">Christian Reiser</a>,
      	  <strong>Songyou Peng</strong>,
      	  <a href="https://yiyiliao.github.io/">Yiyi Liao</a>,
      	  <a href="http://www.cvlibs.net/">Andreas Geiger</a>
      <br>
          <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
      <br>
        <a href="https://arxiv.org/abs/2103.13744">paper</a> |
        <a href="https://creiser.github.io/kilonerf/">project page</a> |
        <a href="https://autonomousvision.github.io/kilonerf/">blog</a> |
        <a href="https://youtu.be/PNh0LvMpovU">video</a> |
        <a href="https://youtu.be/qvsMMDonF28">teaser video</a> |
        <a href="https://github.com/creiser/kilonerf">code</a>
        
        <p></p>
        Over 2000x speed-ups for NeRF are possible by utilizing thousands of tiny MLPs.
        <p></p>
      </td>
    </tr>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    <tr onmouseout="dpcon_stop()" onmouseover="dpcon_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'dpcon_shape'>
        <img src='media/dp-con.png' width="160" height="120"></div>
        <img src='media/dp-con.png' width="160" height="120"></div>
        </div>
        <script type="text/javascript">
        function dpcon_start() { 
        document.getElementById('dpcon_shape').style.opacity = "1";
        }
        function dpcon_stop() { 
        document.getElementById('dpcon_shape').style.opacity = "0"; 
        }
        dpcon_stop()
        </script>
        </script>
      </td>
      <td valign="top" width="75%">
      		<a href="https://github.com/dsvilarkovic/dynamic_plane_convolutional_onet">
            <papertitle>Dynamic Plane Convolutional Occupancy Networks</papertitle></a>
      <br>
      	  <a href="https://www.google.com/search?q=Stefan+Lionar">Stefan Lionar*</a>,
      	  <a href="https://www.google.com/search?q=Daniil+Emtsev">Daniil Emtsev*</a>,
          <a href="https://www.google.com/search?q=Dusan+Svilarkovic">Dusan Svilarkovic*</a>,
          <strong>Songyou Peng</strong>
      <br>
          <em>Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2021
          <br>
   		  (* equal contribution)
      <br>
        <a href="https://arxiv.org/abs/2011.05813">paper</a> |
        <a href="https://screencast-o-matic.com/watch/cYXOcaLKJz">video</a> |
        <a href="https://github.com/dsvilarkovic/dynamic_plane_convolutional_onet">code</a>
        <!-- <a href="https://github.com/B1ueber2y/DIST-Renderer">code</a> -->
        <p></p>
        A student project of 3D Vision course at ETH Zurich where I served as the advisor.
        <p></p>
      </td>
    </tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    <tr onmouseout="con_stop()" onmouseover="con_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'con_shape'>
        <img src='media/con_teaser.gif' width="160" height="120"></div>
        <img src='media/con_teaser_in.png' width="160" height="120"></div>
        </div>
        <script type="text/javascript">
        function con_start() { 
        document.getElementById('con_shape').style.opacity = "1";
        }
        function con_stop() { 
        document.getElementById('con_shape').style.opacity = "0"; 
        }
        con_stop()
        </script>
        </script>
      </td>
      <td valign="top" width="75%">
      		<a href="https://pengsongyou.github.io/conv_onet">
            <papertitle>Convolutional Occupancy Networks</papertitle></a>
      <br>
      	  <strong>Songyou Peng</strong>, 
      	  <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>,
      	  <a href="https://is.tuebingen.mpg.de/person/lmescheder">Lars Mescheder</a>,
          <a href="https://inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a>,
          <a href="http://www.cvlibs.net/">Andreas Geiger</a>
      <br>
          <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020 <strong>(<span style="color:#c20000;">Spotlight</span>, top 5%)</strong>
   
      <br>
        <a href="http://www.cvlibs.net/publications/Peng2020ECCV.pdf">paper</a> |
        <!-- <a href="http://www.cvlibs.net/publications/Peng2020ECCV_supplementary.pdf">supplementary</a> | -->
        <a href="https://pengsongyou.github.io/conv_onet">project page</a> |
        <a href="https://autonomousvision.github.io/convolutional-occupancy-networks/">blog</a> |
        <a href="https://www.youtube.com/watch?v=EmauovgrDSM">video</a> |
        <a href="https://youtu.be/k0monzIcjUo">teaser video</a> |
        <a href="https://github.com/autonomousvision/convolutional_occupancy_networks">code</a><br>
        <a href="https://www.paperdigest.org/2023/04/most-influential-eccv-papers-2023-04/"><b><span style="color:#c20000;">Most influential ECCV'20 papers #13</span></b></a>
        <p></p>
         A flexible implicit representation for accurate large-scale 3D reconstruction.
        <p></p>
      </td>
    </tr>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <!-- <tr onmouseout="wizard_stop()" onmouseover="wizard_start()" bgcolor="#ffffd0"> -->
    <tr onmouseout="dist_stop()" onmouseover="dist_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'dist_shape'>
        <img src='media/dist.gif' width="160" height="100"></div>
        <img src='media/dist.jpg' width="160" height="100"></div>
        </div>
        <script type="text/javascript">
        function dist_start() { 
        document.getElementById('dist_shape').style.opacity = "1";
        }
        function dist_stop() { 
        document.getElementById('dist_shape').style.opacity = "0"; 
        }
        dist_stop()
        </script>
        </script>
      </td>
      <td valign="top" width="75%">
      		<a href="http://b1ueber2y.me/projects/DIST-Renderer/">
            <papertitle>DIST: Rendering Deep Implicit Signed Distance Function with Differentiable Sphere Tracing</papertitle></a>
      <br>
          <a href="http://b1ueber2y.me">Shaohui Liu</a>, 
          <a href="https://www.zhangyinda.com">Yinda Zhang</a>, 
          <strong>Songyou Peng</strong>, 
          <a href="https://ci.idm.pku.edu.cn">Boxin Shi</a>, 
          <a href="https://inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a>,
          <a href="https://zhpcui.github.io/">Zhaopeng Cui</a>
      <br>
          <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
   
      <br>
        <a href="https://arxiv.org/abs/1911.13225">paper</a> |
        <a href="http://b1ueber2y.me/projects/DIST-Renderer/">project page</a> |
        <a href="media/dist-1min.mp4">teaser video</a> |
        <a href="http://b1ueber2y.me/projects/DIST-Renderer/pdf/4986-poster.pdf">poster</a> |
        <a href="https://github.com/B1ueber2y/DIST-Renderer">code</a>
        <p></p>
        A differentiable renderer for deep implicit signed distance functions.
        <p></p>
      </td>
    </tr>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <!-- <tr onmouseout="wizard_stop()" onmouseover="wizard_start()" bgcolor="#ffffd0"> -->
    <tr onmouseout="wizard_stop()" onmouseover="wizard_start()">	
      <td width="25%">
        <div class="one">
        <div class="two" id = 'wizard_shape'>
        <img src='media/wizard2.jpg' width="160" height="110"></div>
        <img src='media/wizard1.jpg' width="160" height="110"></div>
        </div>
        <script type="text/javascript">
        function wizard_start() { 
        document.getElementById('wizard_shape').style.opacity = "1";
        }
        function wizard_stop() { 
        document.getElementById('wizard_shape').style.opacity = "0"; 
        }
        wizard_stop()
        </script>
        </script>
      </td>
      <td valign="top" width="75%">
      		<a href="https://github.com/pengsongyou/CalibrationWizard">
            <papertitle>Calibration Wizard: A Guidance System for Camera Calibration Based on Modelling Geometric and Corner Uncertainty</papertitle></a>
      <br>
          <strong>Songyou Peng</strong> and  
          <a href="https://team.inria.fr/steep/people/peter-sturm/">Peter Sturm</a>
      <br>
          <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019  <strong>(<span style="color:#c20000;">Oral</span>, top 4.6%)</strong>
   
      <br>
        <a href="https://arxiv.org/pdf/1811.03264.pdf">paper</a> |
        <a href="https://youtu.be/my3jocjpD0U?t=398">video</a> |
		<a href="files/iccv19_poster.pdf">poster</a> |
        <a href="https://github.com/pengsongyou/CalibrationWizard">code</a>
        <p></p>
        A novel system that interactively guides a user to take optimal calibration images.
        <p></p>
      </td>
    </tr>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="pami_stop()" onmouseover="pami_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id = 'pami_shape'>
        <img src='media/face_out.jpg' width="160" height="120"></div>
        <img src='media/face_in.jpg' width="160" height="120"></div>
        </div>
        <script type="text/javascript">
        function pami_start() { 
        document.getElementById('pami_shape').style.opacity = "1";
        }
        function pami_stop() { 
        document.getElementById('pami_shape').style.opacity = "0"; 
        }
        pami_stop()
        </script>
        </script>
      </td>
      <td valign="top" width="75%">
      		<a href="https://vision.in.tum.de/data/datasets/photometricdepthsr">
            <papertitle>Photometric Depth Super-Resolution</papertitle></a>
      <br>
          <a href="https://vision.in.tum.de/members/haefner">Bjoern Haefner</a>*, <strong>Songyou Peng*</strong>, 
          <a href="">Alok Verma</a>*, 
          <a href="https://sites.google.com/view/yvainqueau">Yvain Queau</a>, 
          <a href="https://vision.in.tum.de/members/cremers">Daniel Cremers</a> 
      <br>
          <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2019</em>
      <br>
           (* equal contribution)
      <br>
        <a href="https://arxiv.org/abs/1809.10097">paper</a> |
        <a href="https://vision.in.tum.de/data/datasets/photometricdepthsr">project page</a> 
        <p></p>
        Recover high-resolution depth maps with fine geometric details using photometric techniques.
        <p></p>
      </td>
    </tr>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="persemon_stop()" onmouseover="persemon_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id = 'persemon_shape'>
        <img src='media/persemon2.jpg' width="160" height="90"></div>
        <img src='media/persemon2.jpg' width="160" height="90"></div>
        </div>
        <script type="text/javascript">
        function persemon_start() { 
        document.getElementById('persemon_shape').style.opacity = "1";
        }
        function persemon_stop() { 
        document.getElementById('persemon_shape').style.opacity = "0"; 
        }
        persemon_stop()
        </script>
        </script>
      </td>
      <td valign="top" width="75%">
      		<a href="https://github.com/ZhangLeUestc/PersEmoN">
            <papertitle>PersEmoN: A Deep Network for Joint Analysis of Apparent Personality, Emotion and Their Relationship</papertitle>
      <br>
          <a href="https://zhangleuestc.github.io">Le Zhang</a>, <strong>Songyou Peng</strong>, <a href="https://stefan.winkler.site">Stefan Winkler</a>
      <br>
          <em>IEEE Transactions on Affective Computing (<strong>TAFFC</strong>), 2019. In press. </em>
      <br>
        <a href="https://arxiv.org/pdf/1811.08657.pdf">paper</a> |
		<a href="https://github.com/ZhangLeUestc/PersEmoN">code</a>
        <p></p>
        A journal extension of our ACM MM 2018 paper.
        <p></p>
      </td>
    </tr>
  


    
    <tr onmouseout="personality_stop()" onmouseover="personality_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id = 'personality_image'><img src='media/acmmm_demo.gif' width="160" height="110"></div>
        <img src='media/acmmm_demo_still.jpg' width="160" height="110">
        </div>
        <script type="text/javascript">
        function personality_start() {
        document.getElementById('personality_image').style.opacity = "1";
        }
        function personality_stop() {
        document.getElementById('personality_image').style.opacity = "0";
        }
        personality_stop()
        </script>
        </script>
      </td>
      <td valign="top" width="75%">
      		<a href="https://github.com/ZhangLeUestc/PersEmoN">
            <papertitle>Give Me One Portrait Image, I Will Tell You Your Emotion and Personality</papertitle></a>
      <br>
          <strong>Songyou Peng</strong>, <a href="https://zhangleuestc.github.io">Le Zhang</a>, <a href="https://stefan.winkler.site">Stefan Winkler</a>, <a href="http://winslett.cs.illinois.edu/">Marianne Winslett</a>
      <br>
        <em>ACM International Conference on Multimedia (<strong>ACM MM</strong>)</em>, 2018
        <br>
        <a href="files/mm18_personality_paper.pdf">paper</a> |
        <a href="files/mm18_personality_slide.pdf">slides</a> |
        <a href="https://github.com/ZhangLeUestc/PersEmoN">code</a>
        <p></p>
        <p>Technical Demo. A deep Siamese-like network is introduced to predict one's Big-Five personality and arousal-valence emotion from one portrait photo.</p>
      </td>
    </tr>

    <tr onmouseout="iccvw_stop()" onmouseover="iccvw_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id = 'iccvw_shape'><img src='media/iccvw17_pic2.jpg' width="160" height="110"></div>
        <img src='media/iccvw17_pic1.jpg' width="160" height="110"></div>
        <!--<img src='media/selectivity_1.png' width="160" height="130">-->
        </div>
        <script type="text/javascript">
        function iccvw_start() {
        document.getElementById('iccvw_shape').style.opacity = "1";
        }
        function iccvw_stop() {
        document.getElementById('iccvw_shape').style.opacity = "0";
        }
        iccvw_stop()
        </script>
        </script>
      </td>
      <td valign="top" width="75%">
      		<a href="https://vision.in.tum.de/data/datasets/photometricdepthsr">
            <papertitle>Depth Super-Resolution Meets Uncalibrated Photometric Stereo</papertitle></a>
      <br>
          <strong>Songyou Peng</strong>, <a href="https://vision.in.tum.de/members/haefner">Bjoern Haefner</a>, <a href="https://vision.in.tum.de/members/queau">Yvain Queau</a>, <a href="https://vision.in.tum.de/members/cremers">Daniel Cremers</a>
      <br>
        <em>International Conference on Computer Vision (<strong>ICCV</strong>) Workshops</em>, 2017
        <br>
        <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/w43/html/Peng_Depth_Super-Resolution_Meets_ICCV_2017_paper.html">paper</a> |
        <a href="files/iccvw17_presentation.pdf">slides</a> |
        <a href="https://github.com/pengsongyou/SRmeetsPS">code & data</a>
        <p></p>
        <p>A novel depth super-resolution approach for RGB-D sensors is presented.</p>
        <p>This paper a part of my master thesis, and subsumed by our <a href="https://arxiv.org/abs/1809.10097">TPAMI paper</a>.</p>
      </td>
    </tr>

    <tr onmouseout="msc_stop()" onmouseover="msc_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id = 'msc_shape'><img src='media/ratio_shoe_shape.jpg' width="160" height="120"></div>
        <img src='media/ratio_shoe_rgb.jpg' width="160" height="120">
        <!--<img src='media/selectivity_1.png' width="160" height="130">-->
        </div>
        <script type="text/javascript">
        function msc_start() {
        document.getElementById('msc_shape').style.opacity = "1";
        }
        function msc_stop() {
        document.getElementById('msc_shape').style.opacity = "0";
        }
        msc_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p>
        <a href="https://github.com/pengsongyou/msc-thesis">
        <papertitle>High Quality Shape from a RGB-D Camera using Photometric Stereo</papertitle></a>
        <br>
          <strong>Songyou Peng</strong> 
        <br>
        <em>M.Sc. Thesis</em>, Techinical University of Munich<br>
        Supervisor: <a href="https://vision.in.tum.de/members/queau">Yvain Queau</a> and <a href="https://vision.in.tum.de/members/cremers">Daniel Cremers</a> 
        <br>
        <a href="https://github.com/pengsongyou/msc-thesis">thesis</a> |
        <a href="bib/peng2017msc.txt">bibtex</a> |
        <a href="files/msc_poster.pdf">poster</a>
        <p></p>
        <p></p>
      </td>
    </tr>
  
  </table>
    

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>Mentored Students and Interns</heading>
      <tr>
        <td>
          I am fortunate to (co-)mentor some talented and highly motivated students and interns. I have learnt from and gotten inspired by them:
          <ul>
              <li> <a href="https://boyangdeng.com/"><strong>Boyang Deng</strong></a> (2024): PhD student at Stanford University<br>
              <ul>
                <li> Internship project at Google DeepMind: <a href="https://boyangdeng.com/visual-chronicles/">Visual Chronicles</a> (ICCV'25 <span style="color:#c20000;">highlight</span> paper)
              </ul>
              <br>
              <li> <a href="https://boyangdeng.com/"><strong>Anh Thai</strong></a> (2024): PhD student at Georgia Tech<br>
              <ul>
                <li> Internship project at Google DeepMind: <a href="https://arxiv.org/abs/2503.06271">SplatTalk</a> (ICCV'25)
                <li> &#8594; Senior AI Multimodal Researcher at <a href="https://www.dolby.com/">Dolby Laboratories</a>
              </ul>
              <br>
              <li> <a href="https://janackermann.info/"><strong>Jan Ackermann</strong></a> (2024): MSc student at ETH Zurich<br>
              <ul>
                <li> Semester thesis: Continual Learning of Gaussian Splatting with Local Optimization (ICCV'25)
                <li> &#8594; Master thesis and PhD student at Stanford University, advised by <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>
              </ul>
              <br>
              <li> <a href="https://www.linkedin.com/in/goncayilmaz/"><strong>Gonca Yilmaz</strong></a> (2024): MSc student at University of Zurich<br>
                <ul>
                  <li> Semester thesis: Open Vocabulary Segmentation from Multi-Modal Inputs (ICCVW'23)
                  <li> Master thesis: <a href="https://open-das.github.io/">OpenDAS: Open-Vocabulary Domain Adaption for Segmentation</a> (ICLR'25 submission)
                  <li> &#8594; Software engineer at <a href="https://about.google/">Google</a>
                </ul>
              <br>
              <li> <a href="https://www.google.com/search?q=weining+ren"><strong>Weining Ren</strong></a> (2023): MSc student at ETH Zurich<br>
                <ul>
                  <li> Master thesis: <a href="https://rwn17.github.io/nerf-on-the-go/">NeRF On-the-go</a> (CVPR'24)
                  <li> &#8594; PhD student at The University of Hong Kong (HKU), advised by <a href="https://www.kaihan.org/">Kai Han</a>
                </ul>
              <br>
              <li> <a href="https://scholar.google.com/citations?user=A6d9VTAAAAAJ&hl=en"><strong>Lei Li</strong></a> (2023): MSc student at ETH Zurich<br>
                <ul>
                  <li> Master thesis: <a href="https://pengsongyou.github.io">3D Neural Edge Reconstruction</a> (CVPR'24)
                  <li> &#8594; Computer vision engineer at <a href="https://www.bytedance.com/en/">ByteDance</a>
                </ul>
              <br>
              <li> <a href="https://www.linkedin.com/in/mikarimov/"><strong>Mirlan Karimov</strong></a> (2023): MSc student at ETH Zurich<br>
                <ul>
                  <li> Master thesis: Interactive Preprocessing via Multi-Modal Prompting for NeRFs
                  <li> &#8594; PhD student at <a href="https://www.mercedes-benz.de/">Mercedes-Benz AG</a>
                </ul>
              <br>
              <li> <a href="https://junrul.github.io/"><strong>Junru Lin</strong></a> (2023): BSc student at Univeristy of Toronto<br>
                <ul>
                  <li> Summer project: <a href="https://arxiv.org/abs/2312.13332">Ternary-type Opacity and Hybrid Odometry for RGB-only NeRF-SLAM</a> (IROS'24)
                </ul>
              <br>
              <li> <a href="https://primecai.github.io/"><strong>Shengqu Cai</strong></a> (2022): MSc student at ETH Zurich<br>
                <ul>
                  <li> Master thesis: <a href="https://primecai.github.io/diffdreamer">DiffDreamer</a> (ICCV'23)
                  <li> &#8594; PhD student at Stanford University, advised by <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>
                </ul>
              <br>
              <li> <a href="https://zzh2000.github.io/"><strong>Zihan Zhu</strong></a> (2021): BSc student at Zhejiang University<br>
                <ul>
                  <li> Bachelor internship project: <a href="https://pengsongyou.github.io/nice-slam">NICE-SLAM</a> (CVPR'22)
                    <li> Semester project: <a href="https://nicer-slam.github.io/">NICER-SLAM</a> (3DV'24, <strong><span style="color:#c20000;">Best Paper Honorable Mention</span></strong>)
                    <li> Semester project: <a href="https://wildgs-slam.github.io/">WildGS-SLAM</a> (CVPR'25)
                    <li> Master thesis: Leverage geometric constraints for better depth estimation
                  <li> &#8594; Direct doctorate student at ETH Zurich, advised by <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>
                </ul>
              <br>
              <li> <a href="https://www.linkedin.com/in/severin-pfister-772b03ba"><strong>Pfister Severin</strong></a> (2021): MSc student at ETH Zurich<br>
                <ul>
                  <li> Master thesis: Online Implicit Reconstruction
                  <li> &#8594; Consultant at McKinsey & Company
                </ul>
              <br>
              <li> <a href="https://chiaki530.github.io/"><strong>Weirong Chen</strong></a> (2020): MSc student at ETH Zurich<br>
                <ul>
                  <li> Semester thesis: Real-time 3D Reconstruction through Neural Implicit Representation
                  <li> &#8594; PhD student at TU Munich, advised by <a href="https://cvg.cit.tum.de/members/cremers">Daniel Cremers</a> and <a href="https://www.robots.ox.ac.uk/~vedaldi">Andrea Vedaldi</a>
                </ul>
        </td>
      </tr>
  </table>



  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;">
    <heading>Invited Talks</heading>
    <!-- <p></p> -->
    <br><br><br>

    <tr onmouseout="splatacular_stop()" onmouseover="splatacular_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'hku_shape'>
        <img src='media/talk_stanford_2025.jpg' width="180" height="100"></div>
        <img src='media/talk_stanford_2025.jpg' width="180" height="100"></div>
        </div>
        <script type="text/javascript">
        function splatacular_start() { 
        document.getElementById('splatacular_shape').style.opacity = "1";
        }
        function splatacular_stop() { 
        document.getElementById('splatacular_shape').style.opacity = "0"; 
        }
        splatacular_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="files/talk_stanford_2025.pdf">
            <papertitle>A "Splatacular" Year of 3D Reconstruction</papertitle></a>
      <br>
          <em><strong>Stanford University</strong></em>, hosted by <a href="https://ir0.github.io/">Iro Armeni</a>, 2025 <br>
          <em><strong>KAIST</strong></em>, hosted by <a href="https://mhsung.github.io/">Minhyuk Sung</a>, 2025 (<strong><span style="color:#c20000;">Guest Lecture</span></strong>)
      <br>
      <a href="https://youtu.be/pcTNtDSS7p4?feature=shared">recording</a> | <a href="files/talk_hku.pdf">slides</a>
      </td>
    </tr>

    <tr onmouseout="hku_stop()" onmouseover="hku_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'hku_shape'>
        <img src='media/talk_hku_teaser.jpg' width="180" height="100"></div>
        <img src='media/talk_hku_teaser.jpg' width="180" height="100"></div>
        </div>
        <script type="text/javascript">
        function hku_start() { 
        document.getElementById('hku_shape').style.opacity = "1";
        }
        function hku_stop() { 
        document.getElementById('hku_shape').style.opacity = "0"; 
        }
        hku_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="files/talk_hku.pdf">
            <papertitle>2D Magic in a 3D World</papertitle></a>
      <br>
          <em><strong>Imperial College London</strong></em>, hosted by <a href="https://www.doc.ic.ac.uk/~ajd/">Andrew Davison</a>, 2024<br>
          <em><strong>Czech Technical University (CTU)</strong></em>, hosted by <a href="https://tsattler.github.io/">Torsten Sattler</a>, 2024<br>
          <em><strong>The University of Hong Kong (HKU)</strong></em>, hosted by <a href="https://www.kaihan.org/">Kai Han</a>, 2024
      <br>
        <a href="files/talk_hku.pdf">slides</a>
      </td>
    </tr>

    <tr onmouseout="sgp_stop()" onmouseover="sgp_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'sgp_shape'>
        <img src='media/talk_sgp_teaser.jpg' width="180" height="100"></div>
        <img src='media/talk_sgp_teaser.jpg' width="180" height="100"></div>
        </div>
        <script type="text/javascript">
        function sgp_start() { 
        document.getElementById('sgp_shape').style.opacity = "1";
        }
        function sgp_stop() { 
        document.getElementById('sgp_shape').style.opacity = "0"; 
        }
        sgp_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="files/talk_sgp.pdf">
            <papertitle>Dive into Neural Explicit-Implicit 3D Representations and Their Applications</papertitle></a>
      <br>
          <em>Symposium of Geometry Processing (<strong>SGP</strong>) Graduate School</em>, 2023 (<strong><span style="color:#c20000;">Invited Lecture</span></strong>)
      <br>
        <a href="files/talk_sgp.pdf">slides</a>
      </td>
    </tr>

    <tr onmouseout="microsoft_stop()" onmouseover="microsoft_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'microsoft_shape'>
        <img src='media/talk_microsoft_teaser.jpg' width="180" height="100"></div>
        <img src='media/talk_microsoft_teaser.jpg' width="180" height="100"></div>
        </div>
        <script type="text/javascript">
        function microsoft_start() { 
        document.getElementById('microsoft_shape').style.opacity = "1";
        }
        function microsoft_stop() { 
        document.getElementById('microsoft_shape').style.opacity = "0"; 
        }
        microsoft_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="files/talk_microsoft.pdf">
            <papertitle>Learning to Reconstruct and Understand the 3D World</papertitle></a>
      <br>
          <em><strong>Microsoft Mixed Reality & AI Labs - Zurich</strong></em>, 2023
      <br>
        <a href="files/talk_microsoft.pdf">slides</a>
      </td>
    </tr>

    <tr onmouseout="shanghai_stop()" onmouseover="shanghai_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'shanghai_shape'>
        <img src='media/talk_shanghai_teaser.jpg' width="180" height="100"></div>
        <img src='media/talk_shanghai_teaser.jpg' width="180" height="100"></div>
        </div>
        <script type="text/javascript">
        function shanghai_start() { 
        document.getElementById('shanghai_shape').style.opacity = "1";
        }
        function shanghai_stop() { 
        document.getElementById('shanghai_shape').style.opacity = "0"; 
        }
        shanghai_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="files/talk_shanghai.pdf">
            <papertitle>Learning Neural Scene Representations for 3D Reconstruction and Understanding</papertitle></a>
      <br>
          <em><strong>Shanghai AI Lab</strong></em>, 2023
      <br>
        <a href="files/talk_shanghai.pdf">slides</a>
      </td>
    </tr>

    <tr onmouseout="stability_stop()" onmouseover="stability_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'stability_shape'>
        <img src='media/talk_stability_teaser.jpg' width="180" height="100"></div>
        <img src='media/talk_stability_teaser.jpg' width="180" height="100"></div>
        </div>
        <script type="text/javascript">
        function stability_start() { 
        document.getElementById('stability_shape').style.opacity = "1";
        }
        function stability_stop() { 
        document.getElementById('stability_shape').style.opacity = "0"; 
        }
        stability_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="files/talk_stability.pdf">
            <papertitle>OpenScene: 3D Scene Understanding with Open Vocabularies</papertitle></a>
      <br>
          <em><strong>Peking University</strong></em>, hosted by <a href="https://baoquanchen.info/">Baoquan Chen</a>, 2023
          <br>
          <em><strong>Apple</strong></em>, 2023
          <br>
          <em><strong>Stability.ai</strong></em>, 2023
      <br>
        <a href="files/talk_stability.pdf">slides</a>
      </td>
    </tr>

    <tr onmouseout="bosch_stop()" onmouseover="bosch_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'stanford_shape'>
        <img src='media/talk_bosch.jpg' width="180" height="120"></div>
        <img src='media/talk_bosch.jpg' width="180" height="120"></div>
        </div>
        <script type="text/javascript">
        function bosch_start() { 
        document.getElementById('bosch_shape').style.opacity = "1";
        }
        function bosch_stop() { 
        document.getElementById('bosch_shape').style.opacity = "0"; 
        }
        stanford_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="files/talk_cuhksz.pdf">
            <papertitle>How do NeRF and CLIP advance 3D Scene Reconstruction and Understanding</papertitle></a>
      <br>
          <em>Chinese University of Hong Kong (<strong>CUHK</strong>) Shenzhen</em>, 2023
      <br>
          <em>Bosch Center for Artificial Intelligence (<strong>BCAI</strong>)</em>, 2023
      <br>
        <a href="files/talk_stanford.pdf">slides</a>
      </td>
    </tr>

    <tr onmouseout="stanford_stop()" onmouseover="stanford_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'stanford_shape'>
        <img src='media/talk_stanford.jpg' width="180" height="100"></div>
        <img src='media/talk_stanford.jpg' width="180" height="100"></div>
        </div>
        <script type="text/javascript">
        function stanford_start() { 
        document.getElementById('stanford_shape').style.opacity = "1";
        }
        function stanford_stop() { 
        document.getElementById('stanford_shape').style.opacity = "0"; 
        }
        stanford_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="files/talk_stanford.pdf">
            <papertitle>Large-Scale 3D Scene Reconstruction with NeRF</papertitle></a>
      <br>
          <em><strong>Stanford University</strong></em>, hosted by <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>, 2022
      <br>
        <a href="files/talk_stanford.pdf">slides</a>
      </td>
    </tr>
    
    <tr onmouseout="adobe_stop()" onmouseover="adobe_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'adobe_shape'>
        <img src='media/talk_adobe.jpg' width="180" height="100"></div>
        <img src='media/talk_adobe.jpg' width="180" height="100"></div>
        </div>
        <script type="text/javascript">
        function adobe_start() { 
        document.getElementById('adobe_shape').style.opacity = "1";
        }
        function adobe_stop() { 
        document.getElementById('adobe_shape').style.opacity = "0"; 
        }
        adobe_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="files/talk_adobe.pdf">
            <papertitle>Towards Practical Applications of NeRF</papertitle></a>
      <br>
          <em><strong>Adobe Research</strong></em>, hosted by <a href="https://zexiangxu.github.io/">Zexiang Xu</a>, 2022
      <br>
        <a href="files/talk_adobe.pdf">slides</a>
      </td>
    </tr>

      <tr>
        <tr onmouseout="basel_stop()" onmouseover="basel_start()">  
        <td width="25%">
          <div class="one">
          <div class="two" id = 'gsap_shape'>
          <img src='media/talk_basel.jpg' width="180" height="100"></div>
          <img src='media/talk_basel.jpg' width="180" height="100"></div>
          </div>
          <script type="text/javascript">
          function basel_start() { 
          document.getElementById('gsap_shape').style.opacity = "1";
          }
          function basel_stop() { 
          document.getElementById('gsap_shape').style.opacity = "0"; 
          }
          basel_stop()
          </script>
        </td>
        <td valign="top" width="75%">
        <!-- <td style="padding:20px;width:75%;vertical-align:top">   -->
            <a href="files/neural_scene_rep_for_3d_rec.pdf">
              <papertitle>Neural Scene Representations for 3D Reconstruction</papertitle></a>
        <br>
            <em><strong>University of Basel</strong></em>, 2022
        <br>
          <a href="files/neural_scene_rep_for_3d_rec.pdf">slides</a>
          <!-- <a href="https://www.bilibili.com/video/BV1vb4y1n7b8">talk (in Chinese)</a> -->
        </td>
      </tr>

    <tr onmouseout="talking_stop()" onmouseover="talking_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'gsap_shape'>
        <img src='media/talking_papers.jpg' width="180" height="100"></div>
        <img src='media/talking_papers.jpg' width="180" height="100"></div>
        </div>
        <script type="text/javascript">
        function talking_start() { 
        document.getElementById('gsap_shape').style.opacity = "1";
        }
        function talking_stop() { 
        document.getElementById('gsap_shape').style.opacity = "0"; 
        }
        talking_stop()
        </script>
      </td>
      <td valign="top" width="75%">
      <!-- <td style="padding:20px;width:75%;vertical-align:top">   -->
          <a href="https://www.itzikbs.com/shape-as-points-a-differentiable-poisson-solver">
            <papertitle>Shape As Points: A Differentiable Poisson Solver</papertitle></a>
      <br>
          <em><a href="https://talking.papers.podcast.itzikbs.com/"><strong>Talking Papers Podcast</strong></a></em>, 2022
      <br>
        <a href="https://youtu.be/1bZaKno2FFg">video</a> |
        <a href="https://talking.papers.podcast.itzikbs.com/1914034/10135005-songyou-peng-shape-as-points">podcast</a>
      </td>
    </tr>


    <tr onmouseout="gsap_stop()" onmouseover="gsap_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'gsap_shape'>
        <img src='media/games_sap.jpg' width="180" height="100"></div>
        <img src='media/games_sap.jpg' width="180" height="100"></div>
        </div>
        <script type="text/javascript">
        function gsap_start() { 
        document.getElementById('gsap_shape').style.opacity = "1";
        }
        function gsap_stop() { 
        document.getElementById('gsap_shape').style.opacity = "0"; 
        }
        gsap_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="media/sap/SAP_NeurIPS_GAMES.pdf">
            <papertitle>Shape As Points: A Differentiable Poisson Solver</papertitle></a>
      <br>
          <em> Graphics And Mixed Environment Seminar (<strong>GAMES</strong>)</em>, 2021
      <br>
        <a href="media/sap/SAP_NeurIPS_GAMES.pdf">slides</a> |
        <a href="https://www.bilibili.com/video/BV1vb4y1n7b8">talk (in Chinese)</a>
      </td>
    </tr>
  <!-- </table> -->
  <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" > -->
    <tr onmouseout="gnerf_stop()" onmouseover="gnerf_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'gnerf_shape'>
        <img src='media/games_nerf.png' width="160" height="100"></div>
        <img src='media/games_nerf.png' width="160" height="100"></div>
        </div>
        <script type="text/javascript">
        function gnerf_start() { 
        document.getElementById('gnerf_shape').style.opacity = "1";
        }
        function gnerf_stop() { 
        document.getElementById('gnerf_shape').style.opacity = "0"; 
        }
        gnerf_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="files/Towards_Practical_Application_of_NeRF.pdf">
            <papertitle>Towards Practical Applications of NeRF</papertitle></a>
      <br>
          <em> Graphics And Mixed Environment Seminar (<strong>GAMES</strong>)</em>, 2021
      <br>
        <a href="files/Towards_Practical_Application_of_NeRF.pdf">slides</a> |
        <a href="https://www.bilibili.com/video/BV1f54y1H7cY">talk (in Chinese)</a>   
      </td>
    </tr>
  </table>

      <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Selected Projects</heading>
        </td>
      </tr>
      </table> -->
      <table width="100%" align="center" border="0" cellpadding="20">
        <heading>Selected Projects</heading>
      <tr onmouseout="sharp_stop()" onmouseover="sharp_start()">
        <td width="25%">
          <div class="one">
          <div align="center" class="two" id='dna_gif'><img src='media/cvpr2022_logo.png' width="160" height="80"></div>
          <div align="center" class="two" id='dna_image'><img src='media/cvpr2022_logo.png' width="160" height="80"></div>
          </div>
          <script type="text/javascript">
          function sharp_start() {
          document.getElementById('sharp_gif').style.opacity = "1";
          }
          function sharp_stop() {
          document.getElementById('sharp_gif').style.opacity = "0";
          }
          sharp_stop()
          </script>
        </td>
        <td width="75%" valign="top">
        <p>
          <a href="https://arxiv.org/abs/2209.03254">
          <papertitle>3D Textured Shape Recovery with Learned Geometric Priors</papertitle>
          </a>
          <br>
          Lei Li, Zhizheng Liu, Weining Ren, Liudi Yang, <a href="https://fangjinhuawang.github.io/">F. Wang</a>, <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>, <strong>Songyou Peng</strong>
          <br>
    	  <em>Shape Recovery from Partial Textured 3D Scans (<strong>SHARP</strong>)</em>, 2022<br>
          <a href="https://codalab.lisn.upsaclay.fr/competitions/4603#results">leaderboard</a> |
      	  <a href="https://arxiv.org/abs/2209.03254">arxiv</a> |
          <a href="https://github.com/BoSmallEar/sharp2022">code</a>
          <p></p>
          <p></p>

           <strong>1st</strong> place in reconstructing partial textured objects and <strong>2nd</strong> overall. </a>
        </p>
        </p>
        </td>
      </tr>
      <tr onmouseout="sharp_stop()" onmouseover="sharp_start()">
        <td width="25%">
          <div class="one">
          <div align="center" class="two" id='dna_gif'><img src='media/OMG_logo.png' width="150" height="80"></div>
          <div align="center" class="two" id='dna_image'><img src='media/OMG_logo.png' width="150" height="80"></div>
          </div>
          <script type="text/javascript">
          function sharp_start() {
          document.getElementById('sharp_gif').style.opacity = "1";
          }
          function sharp_stop() {
          document.getElementById('sharp_gif').style.opacity = "0";
          }
          sharp_stop()
          </script>
        </td>
          <td width="75%" valign="top">
          <p>
            <a href="https://arxiv.org/abs/1805.00638">
            <papertitle>A Deep Network for Arousal-Valence Emotion Prediction with Acoustic-Visual Cues</papertitle>
            </a>
            <br>
            <strong>Songyou Peng</strong>, <a href="https://zhangleuestc.github.io">Le Zhang</a>, <a href="https://team.inria.fr/perception/team-members/yutong-ban/">Yutong Ban</a>, <a href="http://people.eng.unimelb.edu.au/mengf1/">Meng Fang</a>, <a href="stefan.winkler.site">Stefan Winkler</a>
            <br>
          <em>IJCNN One-Minute Gradual (OMG) Emotion Behavior Challenge</em>, 2018<br>
            <a href="https://www2.informatik.uni-hamburg.de/wtm/omgchallenges/omg_emotion2018_results2018.html">leaderboard</a> |
            <a href="https://arxiv.org/abs/1805.00638">arxiv</a> |
            <a href="https://github.com/pengsongyou/OMG-ADSC">code</a>
            <p></p>
            <p></p>
  
             <strong>1st</strong> for vision-only arousal/valence prediction and <strong>2nd</strong> for overall valence prediction. </a>
          </p>
          </p>
          </td>
        </tr>
    <tr onmouseout="pepper_stop()" onmouseover="pepper_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id = 'pepper_gif'><img src='media/pepper_robot.gif' width="160" height="120"></div>
        <img src='media/pepper_pic.jpg' width="160" height="120">
        </div>
        <script type="text/javascript">
        function pepper_start() {
        document.getElementById('pepper_gif').style.opacity = "1";
        }
        function pepper_stop() {
        document.getElementById('pepper_gif').style.opacity = "0";
        }
        pepper_stop()
        </script>
        </script>
      </td>
        <!--<td width="25%"><img src="media/gwas.png" alt="GWAS image from https://upload.wikimedia.org/wikipedia/commons/5/59/GWAS_loci_influencing_human_facial_and_scalp_hair_phenotypes_-_cropped.png" width="160" height="160"></td>-->
      <!--<tr>
        <td width="25%"><img src="media/benedict_1.png" alt="benedict-otter-look-a-like" width="160" height="160"></td>-->
        <td width="75%" valign="top">
        <p>
          <a href="https://github.com/PaolaArdon/Salt-Pepper">
          <papertitle>A Hybrid SLAM and Object Recognition System for Pepper Robot</papertitle>
          </a>
          <br>
          <strong>Songyou Peng*</strong>, <a href="https://kz.linkedin.com/in/kushibar">Kaisar Kushibar*</a>, <a href="https://www.edinburgh-robotics.org/students/paola-ardon-ramirez">Paola Ardon*</a>
          <br>
    	  <em>VIBOT Robotics Project</em>, 2016<br>
    	  <a href="https://arxiv.org/abs/1903.00675">arxiv</a> |
          <a href="https://www.youtube.com/watch?v=evFsnWH_bpY">video</a> |
          <a href="https://github.com/pengsongyou/msc-thesis">code</a>
          <p></p>
          <p></p>
          Apply visual SLAM on the Pepper robot along with object recognition.</a>
        </p>
        </p>
        </td>
      </tr>
      </table>
      
      <!-- Teaching -->
      <heading>Teaching</heading>
      <br><br><br>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
          <td style="padding:0px;width:25%;vertical-align:middle">
            <img src="media/games003.jpg" width="180">
          </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            Lecturer, <a href="https://pengsida.net/games003"><strong>GAMES003: 图形视觉科研基本素养 (How To Do Research in CV/CG)</strong></a>, Fall 2024
            <br><br>
            Together with <a href="https://pengsida.net/">Sida Peng</a>, <a href="https://www.cs.toronto.edu/~jungao/">Jun Gao</a>, and <a href="https://qianqianwang68.github.io/">Qianqian Wang</a>.
          </td>
        </tr>
      </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="media/eth_logo.png" width="180">
            </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
              Teaching Assistant (Lead), <a href="https://www.cvg.ethz.ch/teaching/3dvision/"><strong>3D Vision</strong></a>, Spring 2023
                <br>
              Teaching Assistant, <a href="https://cvg.ethz.ch/teaching/compvis/"><strong>Computer Vision</strong></a>, Fall 2022
              <br>
              Teaching Assistant (Lead), <a href="https://www.cvg.ethz.ch/teaching/3dvision/2022/index.php"><strong>3D Vision</strong></a>, Spring 2022
              <br>
              Teaching Assistant, <a href="https://www.cvg.ethz.ch/teaching/dlseminar/"><strong>Deep Learning for Computer Vision: Seminal Work</strong></a>, Spring 2022
              <br>
              Teaching Assistant, <a href="https://www.cvg.ethz.ch/teaching/3dvision/2020/index.php"><strong>3D Vision</strong></a>, Spring 2020
              <br>
              Teaching Assistant, <a href="https://www.cvg.ethz.ch/teaching/dlseminar/2020/"><strong>Deep Learning for Computer Vision: Seminal Work</strong></a>, Spring 2020
              <br>
            </td>
          </tr>
        </table>
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="media/tue_logo.png" width="180">
            </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
              Teaching Assistant, <a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/lectures/deep-learning/"><strong>Deep Learning</strong></a>, Winter 2020/2021
              <br>
            </td>
          </tr>
        </table>
      <br>
      <br>
      <br>

      <!-- Academic Services -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <heading>Academic Services</heading>
          <tr>
            <td>
              
              <ul>
                  <li> <strong>Publicity Chair</strong>: 3DV'25<br>
                  <li> <strong>Area Chair</strong>: ICCV'25, ICML'25, 3DV'24<br>
                  <li> <strong>Workshop Organizer</strong>: <a href="https://opensun3d.github.io/">OpenSUN3D</a> at ICCV'23 (<a href="https://opensun3d.github.io/index_iccv23.html">1st</a>), CVPR'24 (<a href="https://opensun3d.github.io/index_cvpr2024.html">2nd</a>), and ECCV'24 (<a href="https://opensun3d.github.io/index.html">3rd</a>), 1st <a href="https://focus-workshop.github.io/">FOCUS</a> at ECCV'24, <a href="https://scene-understanding.com/">5th Workshop in 3D Scene Understanding</a> at CVPR'25<br>
                  <li> <strong>Conference Reviewer</strong>: CVPR, ICCV, ECCV, ICLR, NeurIPS, SIGGRAPH, SIGGRAPH Asia<br>
                  <li> <strong>Journal Reviewer</strong>: TPAMI, IJCV, CVIU
              </ul>
            </td>
          </tr>
      </table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          template adapted from <a href="https://jonbarron.info/"><font size="2">this awesome website</font></a>
          <!-- <br> -->
          <!-- Last updated: Mar 2023 -->
        </font>
        </p>
        </td>
      </tr>
      </table>
      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-116734954-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116734954-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116734954-1');
</script>
    </td>
    </tr>
  </table>
  </body>
</html>
<!--  -->
